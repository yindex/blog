<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>Hadoop Streaming手册[译] | 数据与安全技术研究</title>
    
    
        <meta name="keywords" content="hadoop,streaming" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="概述hadoop streaming是hadoop发行版附带的功能. 该功能让我们可以使用任何程序或脚本作为mapper和reducer 来创建Map&#x2F;Reduce作业.例如： 12345hadoop jar hadoop-streaming-2.9.2.jar \	-input inputDirs \	-output outputDirs \	-mapper &#x2F;bin&#x2F;cat \	-redu">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop Streaming手册[译]">
<meta property="og:url" content="https://blog.yindex.org/2019/04/01/2019-04-01-Hadoop%20Streaming[%E8%AF%91]/index.html">
<meta property="og:site_name" content="数据与安全技术研究">
<meta property="og:description" content="概述hadoop streaming是hadoop发行版附带的功能. 该功能让我们可以使用任何程序或脚本作为mapper和reducer 来创建Map&#x2F;Reduce作业.例如： 12345hadoop jar hadoop-streaming-2.9.2.jar \	-input inputDirs \	-output outputDirs \	-mapper &#x2F;bin&#x2F;cat \	-redu">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-04-01T00:00:00.000Z">
<meta property="article:modified_time" content="2020-08-01T07:37:54.561Z">
<meta property="article:author" content="yindex">
<meta property="article:tag" content="hadoop">
<meta property="article:tag" content="streaming">
<meta name="twitter:card" content="summary">
    

    
        <link rel="alternate" href="/atom.xml" title="数据与安全技术研究" type="application/atom+xml" />
    

    
        <link rel="icon" href="/favicon.ico" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">

    
<script src="/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
<meta name="generator" content="Hexo 4.2.0"></head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">数据与安全技术研究</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">首页</a>
                
                    <a class="main-nav-link" href="/archives">归档</a>
                
                    <a class="main-nav-link" href="/about">关于</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">首页</a></td>
                
                    <td><a class="main-nav-link" href="/archives">归档</a></td>
                
                    <td><a class="main-nav-link" href="/about">关于</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>recent</span></h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-title"><a href="/2020/08/01/2016-07-07-Java%E9%94%81%E7%B1%BB%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8/" class="title"></a></p>
                            <p class="item-date"><time datetime="2020-08-01T07:37:54.561Z" itemprop="datePublished">2020-08-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-title"><a href="/2019/07/10/go-lvs-toa-ipv4/" class="title">LVS后端GO-Web获取用户IP</a></p>
                            <p class="item-date"><time datetime="2019-07-10T16:12:31.000Z" itemprop="datePublished">2019-07-10</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-title"><a href="/2019/04/20/2019-04-20-NSQ-%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-TOPIC/" class="title">NSQ代码阅读与分析之TOPIC</a></p>
                            <p class="item-date"><time datetime="2019-04-20T00:00:00.000Z" itemprop="datePublished">2019-04-20</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-title"><a href="/2019/04/20/2019-04-20-NSQ-%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-inFlightPqueue%E5%92%8CPriorityQueue/" class="title">NSQ代码阅读与分析之inFlightPqueue与inFlightPqueue</a></p>
                            <p class="item-date"><time datetime="2019-04-20T00:00:00.000Z" itemprop="datePublished">2019-04-20</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-title"><a href="/2019/04/15/2019-04-20-NSQ-%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-Channel/" class="title">NSQ代码阅读与分析之Channel</a></p>
                            <p class="item-date"><time datetime="2019-04-15T00:00:00.000Z" itemprop="datePublished">2019-04-15</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>


    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>tag cloud</span></h3>
        <div class="widget">
            <a href="/tags/Channel/" style="font-size: 10px;">Channel</a> <a href="/tags/arm/" style="font-size: 13.33px;">arm</a> <a href="/tags/cnn/" style="font-size: 10px;">cnn</a> <a href="/tags/go/" style="font-size: 10px;">go</a> <a href="/tags/golang/" style="font-size: 20px;">golang</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/http/" style="font-size: 10px;">http</a> <a href="/tags/lua/" style="font-size: 10px;">lua</a> <a href="/tags/lvs/" style="font-size: 10px;">lvs</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/nginx/" style="font-size: 10px;">nginx</a> <a href="/tags/nsq/" style="font-size: 20px;">nsq</a> <a href="/tags/nsqlookup/" style="font-size: 10px;">nsqlookup</a> <a href="/tags/streaming/" style="font-size: 10px;">streaming</a> <a href="/tags/tcp6/" style="font-size: 10px;">tcp6</a> <a href="/tags/topic/" style="font-size: 10px;">topic</a> <a href="/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/" style="font-size: 16.67px;">中间件</a> <a href="/tags/%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/" style="font-size: 13.33px;">交叉编译</a> <a href="/tags/%E5%BB%B6%E8%BF%9F%E6%8A%95%E9%80%92/" style="font-size: 10px;">延迟投递</a> <a href="/tags/%E6%B5%B7%E6%80%9D/" style="font-size: 13.33px;">海思</a> <a href="/tags/%E6%B6%88%E6%81%AF%E6%8A%95%E9%80%92/" style="font-size: 10px;">消息投递</a> <a href="/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/" style="font-size: 20px;">消息队列</a> <a href="/tags/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/" style="font-size: 10px;">装饰器模式</a>
        </div>
    </div>


    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>archives</span></h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">1</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>categories</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            hadoop
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file active"><a href="/2019/04/01/2019-04-01-Hadoop%20Streaming%5B%E8%AF%91%5D/">Hadoop Streaming手册[译]</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            lua
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2018/12/28/2018-12-28-%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%20lua%20for%20hisi-arm-linux/">交叉编译 lua for hisi-arm-linux</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            nginx
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2018/12/28/2018-12-31-%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91nginx-1.14.2%20for%20hisi-arm-linux/">交叉编译nginx-1.14.2 for hisi-arm-linux</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            中间件
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2019/03/10/2019-03-10-NSQ%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-nsqlookupd%E7%BB%84%E4%BB%B6/">NSQ源码阅读-nsqlookupd组件</a></li>  <li class="file"><a href="/2019/04/14/2019-04-20-NSQ%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-%E5%BB%B6%E8%BF%9F%E6%B6%88%E6%81%AF/">NSQ延迟消息投递</a></li>  <li class="file"><a href="/2019/04/15/2019-04-20-NSQ-%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-Channel/">NSQ代码阅读与分析之Channel</a></li>  <li class="file"><a href="/2019/04/20/2019-04-20-NSQ-%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-inFlightPqueue%E5%92%8CPriorityQueue/">NSQ代码阅读与分析之inFlightPqueue与inFlightPqueue</a></li>  <li class="file"><a href="/2019/04/20/2019-04-20-NSQ-%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-TOPIC/">NSQ代码阅读与分析之TOPIC</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            后端开发
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2019/07/10/go-lvs-toa-ipv4/">LVS后端GO-Web获取用户IP</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            深度学习
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2017/07/05/%E5%9F%BA%E4%BA%8Ecnn%E8%BF%9B%E8%A1%8Cmnist%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB/">基于CNN的MNIST手写识别尝试</a></li>  </ul> 
                    </li> 
                     <li class="file"><a href="/2020/08/01/2016-07-07-Java%E9%94%81%E7%B1%BB%E5%9E%8B%E5%8F%8A%E5%BA%94%E7%94%A8/"></a></li>  </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>tags</span></h3>
        <div class="widget">
            <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Channel/" rel="tag">Channel</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/arm/" rel="tag">arm</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cnn/" rel="tag">cnn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/go/" rel="tag">go</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/golang/" rel="tag">golang</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/http/" rel="tag">http</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lua/" rel="tag">lua</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lvs/" rel="tag">lvs</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mnist/" rel="tag">mnist</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/" rel="tag">nginx</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nsq/" rel="tag">nsq</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nsqlookup/" rel="tag">nsqlookup</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/streaming/" rel="tag">streaming</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tcp6/" rel="tag">tcp6</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/topic/" rel="tag">topic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/" rel="tag">中间件</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/" rel="tag">交叉编译</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BB%B6%E8%BF%9F%E6%8A%95%E9%80%92/" rel="tag">延迟投递</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%B7%E6%80%9D/" rel="tag">海思</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B6%88%E6%81%AF%E6%8A%95%E9%80%92/" rel="tag">消息投递</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/" rel="tag">消息队列</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/" rel="tag">装饰器模式</a><span class="tag-list-count">1</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap widget-list">
        <h3 class="widget-title"><span>links</span></h3>
        <div class="widget">
            <ul>
                
            </ul>
        </div>
    </div>


    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-2019-04-01-Hadoop Streaming[译]" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/hadoop/" rel="tag">hadoop</a>, <a class="tag-link" href="/tags/streaming/" rel="tag">streaming</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2019/04/01/2019-04-01-Hadoop%20Streaming%5B%E8%AF%91%5D/">
            <time datetime="2019-04-01T00:00:00.000Z" itemprop="datePublished">2019-04-01</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a href='https://github.com/yindex/blog/raw/master/source/_posts/2019-04-01-Hadoop Streaming[译].md' target="_blank" rel="noopener"> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/yindex/blog/edit/master/source/_posts/2019-04-01-Hadoop Streaming[译].md' target="_blank" rel="noopener"> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/yindex/blog/commits/master/source/_posts/2019-04-01-Hadoop Streaming[译].md' target="_blank" rel="noopener"> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            Hadoop Streaming手册[译]
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
        
        
            <!-- toc -->

<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>hadoop streaming是hadoop发行版附带的功能. 该功能让我们可以使用任何程序或脚本作为mapper和reducer 来创建Map/Reduce作业.例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-streaming-2.9.2.jar \</span><br><span class="line">	-input inputDirs \</span><br><span class="line">	-output outputDirs \</span><br><span class="line">	-mapper /bin/cat \</span><br><span class="line">	-reducer /usr/bin/wc</span><br></pre></td></tr></table></figure>

<h2 id="Streaming运行原理"><a href="#Streaming运行原理" class="headerlink" title="Streaming运行原理"></a>Streaming运行原理</h2><p>上述例子中，mapper和reducer都是执行程序，该执行程序从标准输入(stdin)逐行读取数据并将数据吐到标准输出(stdout)中.hadoop streaming创建Map/Reduce作业、提交到对应的集群中并监控作业运行进度.</p>
<p>指定一个可执行程序作为mapper并初始化成功后，每个mapper任务将启动该执行程序.　当mapper作业运行时，它讲输入数据转化成行并逐行写入到该执行程序进程的标准输入中.同时mapper作业面向行收集该执行程序进程的标准输出(以\n作为分割符切分数据)并将每行数据转成成key/value格式，将转化后的kv数据作为mapper作业的输出. 默认配置下，以tab作为分割符，每行数据分割后第一部分作为key其余部分作为value.如果行数据中没有制表符那么整行数据作为key, value为空. 我们可以通过设置-inputformat命令行参数来个性化定制key value格式．</p>
<p>这是Map/Reduce框架与streaming mapper/reduce通信协议的基础.</p>
<p>用户可以通过设置 <code>stream.non.zero.exit.is.failure</code>参数（true|false）来确定streaming作业是以非零状态退出表示作业运行成功或失败．默认情况下,streaming　作业进程以非零状态退出表示任务失败.</p>
<h2 id="Streaming-命令行选项"><a href="#Streaming-命令行选项" class="headerlink" title="Streaming 命令行选项"></a>Streaming 命令行选项</h2><p>Streaming 支持hadoop通用命令行选项的同时也支持streaming命令行选项，下面展示了常规命令行选项的语法格式.</p>
<p><strong>注意：</strong>　一定要把通用命令行选项设置放置在streaming命令行选项之前，否则命令行选项设置将无效. </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop <span class="built_in">command</span> [genericOptions] [streamingOptions]</span><br></pre></td></tr></table></figure>

<p><strong>Ｈadoop streaming 命令行选项列表如下.</strong></p>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="center">必选</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">-input 目录名或文件名</td>
<td align="center">是</td>
<td>mapper作业输入数据位置</td>
</tr>
<tr>
<td align="left">-output 目录名</td>
<td align="center">是</td>
<td>reducer作业输出位置</td>
</tr>
<tr>
<td align="left">-mapper 执行程序或Java类名</td>
<td align="center">否</td>
<td>Mapper执行程序，若未设置则默认使用IdentityMapper</td>
</tr>
<tr>
<td align="left">-reducer 执行程序或Java类名</td>
<td align="center">否</td>
<td>Reducer执行程序，若未设置则默认使用IdentityReducer</td>
</tr>
<tr>
<td align="left">-file 文件名</td>
<td align="center">否</td>
<td>让mapper,reducer,combiner作业能够在对应节点上本地化读取该文件</td>
</tr>
<tr>
<td align="left">-inputformat JavaClassName</td>
<td align="center">否</td>
<td>设置的类必须返回 key(Text类型)/Values(Text类型)键值对．若未设置该选项则默认使用TextInputFormat</td>
</tr>
<tr>
<td align="left">-outputformat JavaClassName</td>
<td align="center">否</td>
<td>同上</td>
</tr>
<tr>
<td align="left">-partioner JavaClassName</td>
<td align="center">否</td>
<td>该类确定输出数据数据写入到哪一个reduce分区</td>
</tr>
<tr>
<td align="left">-combiner streaming命令或JavaClassName</td>
<td align="center">否</td>
<td></td>
</tr>
<tr>
<td align="left">-cmdenv name=value</td>
<td align="center">否</td>
<td>给streaming作业传递环境变量</td>
</tr>
<tr>
<td align="left">-inputreader</td>
<td align="center">否</td>
<td></td>
</tr>
<tr>
<td align="left">-verbose</td>
<td align="center">否</td>
<td>verbose输出</td>
</tr>
<tr>
<td align="left">-lazyOutput</td>
<td align="center">否</td>
<td></td>
</tr>
<tr>
<td align="left">-numReduceTasks</td>
<td align="center">否</td>
<td>reducer作业数量(mapreduce.job.reduces)</td>
</tr>
<tr>
<td align="left">-mapdebug</td>
<td align="center">否</td>
<td>map作业失败时调用的脚本</td>
</tr>
<tr>
<td align="left">-reducedebug</td>
<td align="center">否</td>
<td>reduce作业失败时调用的脚本</td>
</tr>
</tbody></table>
<p>###指定Java类作为Mapper/Reducer</p>
<p>我们可以提供一个Java类作为mapper或者reducer</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-streaming-2.9.2.jar \</span><br><span class="line">-input inputdirs \</span><br><span class="line">-output outputdir \</span><br><span class="line">-inputformat org.apache.hadoop.mapred.KeyValueTextInputFormat \</span><br><span class="line">-mapper org.apache.hadoop.mapred.lib.IdentifyMapper \</span><br><span class="line">-reducer /usr/bin/wc</span><br></pre></td></tr></table></figure>

<p>我们可以通过设置<code>stream.non.zero.exit.is.failure</code>为<code>false</code>或<code>true</code>来确定是否以执行程序退出非零状态码来表示任务成功或失败. 默认程序非零退出状态表示任务失败.</p>
<p>###文件打包到作业提交</p>
<p>我们可以执行任意执行程序作为mapper或reducer.可执行程序并不需要预先保存在集群上;若未提前保存在集群节点中,则需要通过<code>-file</code>选项通知框架打包相应执行程序作为作业提交的一部分. 例如:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-streaming-2.9.2.jar \</span><br><span class="line">-input inputdirs \</span><br><span class="line">-output outputdir \</span><br><span class="line">-mapper mapper.py \</span><br><span class="line">-reducer /usr/bin/wc \</span><br><span class="line">-file mapper.py</span><br></pre></td></tr></table></figure>

<p>上如例子中指定了一个用户定义的python 可执行脚本作为mapper. <code>-file mapper.py</code> 将python可执行文件作为作业提交的一部分传输到集群.除了打包可执行文件,还可以打包其他辅助文件(比如字典文件,配置文件等)供mapper作业使用. 如下所示:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-streaming-2.9.2.jar \</span><br><span class="line">  -input myInputDirs \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper myPythonScript.py \</span><br><span class="line">  -reducer /usr/bin/wc \</span><br><span class="line">  -file myPythonScript.py \</span><br><span class="line">  -file myDictionary.txt</span><br></pre></td></tr></table></figure>
<p>###为作业指定其他插件</p>
<p>跟其他普通Map/Reduce作业一样,我们可以指定其他插件到streaming作业.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-inputformat JavaClassName</span><br><span class="line">-outputformat JavaClassName</span><br><span class="line">-partitioner JavaClassName</span><br><span class="line">-combiner streamingCommand or JavaClassName</span><br></pre></td></tr></table></figure>

<p>给输入格式指定的Java类必须返回Text 格式的键值对类型. 若没有特别指定输入格式处理类,那么<code>TextInputFormat</code>将作为默认处理类.由于<code>TextInputFormat</code> 返回LongWriteable类型key, 并且key不是输入数据的一部分,key会被丢弃.只有值会通过管道传递给streaming mapper.</p>
<p>为输出格式指定的类应该采用<code>Text</code>类型对键值对类型.如果没有指定输出格式处理类,<code>TextOutputFormat</code>作为默认处理类.</p>
<p>###设置环境变量</p>
<p>通过下述方式再streaming命令行中设置环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-cmdenv EXAMPLE_DIR=/home/example/dic/</span><br></pre></td></tr></table></figure>



<h2 id="通用命令行选项"><a href="#通用命令行选项" class="headerlink" title="通用命令行选项"></a>通用命令行选项</h2><p>Hadoop streaming支持streaming命令行选项也支持hadoop通用命令行选项.通用命令行选项设置方式如下</p>
<p><strong>注意:</strong>  一定要把通用命令行选项设置放置在streaming命令行选项之前，否则命令行选项设置将无效. </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop <span class="built_in">command</span> [genericOptions] [streamingOptions]</span><br></pre></td></tr></table></figure>

<p>Streaming可以使用的hadoop命令行选项如下表所示:</p>
<table>
<thead>
<tr>
<th>参数</th>
<th align="center">必须</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>-conf configfile</td>
<td align="center">否</td>
<td>指定特定的配置文件</td>
</tr>
<tr>
<td>-D</td>
<td align="center">否</td>
<td>设置指定的属性(Dkey=value)</td>
</tr>
<tr>
<td>-fs host:port or local</td>
<td align="center">否</td>
<td>指定namenode</td>
</tr>
<tr>
<td>-files</td>
<td align="center">否</td>
<td>指定需要copy到集群的文件,多个文件用逗号分开</td>
</tr>
<tr>
<td>-libjars</td>
<td align="center">否</td>
<td>指定需要copy到集群classpath的jar文件,多个文件用逗号分开</td>
</tr>
<tr>
<td>-archives</td>
<td align="center">否</td>
<td>指定要上传到集群并解压到压缩文件,多个文件用逗号分开</td>
</tr>
</tbody></table>
<p>###使用-D选项设置配置变量</p>
<p>我们可以使用`-D<property>=<value>来设置额外的配置.</p>
<h4 id="设置目录"><a href="#设置目录" class="headerlink" title="设置目录"></a>设置目录</h4><p>修改节点本地临时目录路径</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-D dfs.data.dir=/tmp</span><br></pre></td></tr></table></figure>

<p>设置额外的本地临时目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-D mapred.local.dir=/tmp/<span class="built_in">local</span></span><br><span class="line">-D mapred.system.dir=/tmp/system</span><br><span class="line">-D mapred.temp.dir=/tmp/temp</span><br></pre></td></tr></table></figure>

<p><strong>注释:</strong> 作业配置参数跟多细节请参考<code>mapred-default.xml</code></p>
<p>####设置仅运行Map任务作业</p>
<p>我们经常需要仅运行map函数来处理输入数据,仅仅需要把<code>mapreduce.job.reduces</code>设置为0即可达到此目的.设置为0后,Map/Reduce框架不再创建reduce任务,并且mapper任务等输出会作为作业的最终输出.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-D mapreduce.job.reduces=0</span><br></pre></td></tr></table></figure>

<p>####设置指定数量的Reducers</p>
<p>设置指定数量的reducers,例如设置2个:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoo-streaming-2.9.2.jar \</span><br><span class="line">-D mapreduce.job.reduces=2\</span><br><span class="line">-input inputdir \</span><br><span class="line">-output outputdir \</span><br><span class="line">-mapper /bin/cat \</span><br><span class="line">-reducer /usr/bin/wc</span><br></pre></td></tr></table></figure>

<h4 id="自定义输入行分割键值对的方式"><a href="#自定义输入行分割键值对的方式" class="headerlink" title="自定义输入行分割键值对的方式"></a>自定义输入行分割键值对的方式</h4><p>如前面讲到,Map/Reduce框架从mapper任务的标准输出读取行数据时,它会把行数据分割成呢键值对形式.默认配置下,到第一个制表符的数据作为键,其余的数据作为值(不包含制表符tab).</p>
<p>但是,我们可以修改默认配置.我们可以指定一个分割符号来替换制表符.并且也可以值指定分割后的哪一部分作为key.如下所示</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-streaming-2.9.2.jar \</span><br><span class="line">  -D stream.map.output.field.separator=. \</span><br><span class="line">  -D stream.num.map.output.key.fields=4 \</span><br><span class="line">  -input myInputDirs \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper /bin/cat \</span><br><span class="line">  -reducer /bin/cat</span><br></pre></td></tr></table></figure>

<p>上面的例子中,通过<code>-D stream.map.output.field.separator=.</code>设置<code>.</code>作为map输出的字段分割符.并且指定到第四个分割符的前缀部分作为key,其余部分作为值.如果一行数据中的风格符号<code>.</code>不够4个那么整行数据讲作为key,值部分为空.</p>
<p>类似的,我们可以使用<code>-D stream.reduce.output.field.separator=SEP</code>和 <code>-D stream.num.reduce.output.fields=NUM</code> 来确定reduce的输出数据中哪一部分作为key哪一部分作为value.</p>
<p>类似上述方式,可以通过<code>stream.map.input.field.separato</code> 和<code>stream.reduce.input.field.separator</code>来设置Map/Reduce作业的输入数据分割符号.默认设置是以制表符作为字段分割符号.</p>
<h3 id="使用大文件和归档文件"><a href="#使用大文件和归档文件" class="headerlink" title="使用大文件和归档文件"></a>使用大文件和归档文件</h3><p><code>-files</code>和<code>-archives</code>选项让我们可以在任务中使用普通文件和归档文件.它们的参数是指向上传到hdfs的文件或归档文件到URI.这些文件缓存在作业中.我们可以通过fs.default.name配置变量获取host和fs_port.</p>
<p><strong>注意:</strong> -files和-archives选项是hadoop通用命令行选项,一定要把它们放置在hadoop streaming选项之前,否则设置无效. </p>
<h4 id="让任务可访问文件"><a href="#让任务可访问文件" class="headerlink" title="让任务可访问文件"></a>让任务可访问文件</h4><p>-files选项在当前任务的工作目录创建指向本地文件副本的符号连接.下面例子中hadoop在当前工作目录中自动创建了名为testfile.txt的符号链接.链接指向了本地testfile.txt副本.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-files hdfs://host:fs_port/usr/testfile.txt</span><br></pre></td></tr></table></figure>

<p>也可以使用<code>#</code>来为-files 指定的文件创建一个不同名称的符号链接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-files hdfs://host:fs_port/user/testfile.txt<span class="comment">#cachefile.txt</span></span><br></pre></td></tr></table></figure>

<p>如果有多个文件可以用逗号分开,如下所示</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-files /home/usr/testfile1.txt, /home/user/estfile2.txt</span><br></pre></td></tr></table></figure>

<h4 id="任务访问归档文件"><a href="#任务访问归档文件" class="headerlink" title="任务访问归档文件"></a>任务访问归档文件</h4><p>-archives选项让我们可以把本地jars文件复制到当前任务的工作目录并unjar(解压).</p>
<p>下面例子中,hadoop在任务当前工作目录中创建了一个名为testfile.jar的符号链接.这个符号链接指向了上传的jar文件解压后的目录.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-archives hdfs://host:fs_port/user/testfile.jar</span><br></pre></td></tr></table></figure>

<p>使用<code>#</code>符号为-archives上传文件、解压后的目录创建一个不同名字的符号链接.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-archives hdfs://host:fs_port/user/testfile.tgz<span class="comment">#tgzdir</span></span><br></pre></td></tr></table></figure>
<p>下面例子中,input.txt包含两行数据,分别表示两个文件: cachedir.jar/cache.txt 和 cacheddir.jar/cache2.txt.  cacheddir.jar是指向包含cache.txt和cache2.txt文件的归档目录的一个符号链接.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-streaming-2.9.2.jar \</span><br><span class="line">                -archives <span class="string">'hdfs://hadoop-nn1.example.com/user/me/samples/cachefile/cachedir.jar'</span> \</span><br><span class="line">                -D mapreduce.job.maps=1 \</span><br><span class="line">                -D mapreduce.job.reduces=1 \</span><br><span class="line">                -D mapreduce.job.name=<span class="string">"Experiment"</span> \</span><br><span class="line">                -input <span class="string">"/user/me/samples/cachefile/input.txt"</span> \</span><br><span class="line">                -output <span class="string">"/user/me/samples/cachefile/out"</span> \</span><br><span class="line">                -mapper <span class="string">"xargs cat"</span> \</span><br><span class="line">                -reducer <span class="string">"cat"</span></span><br><span class="line"></span><br><span class="line">$ ls test_jar/</span><br><span class="line">cache.txt  cache2.txt</span><br><span class="line"></span><br><span class="line">$ jar cvf cachedir.jar -C test_jar/ .</span><br><span class="line">added manifest</span><br><span class="line">adding: cache.txt(<span class="keyword">in</span> = 30) (out= 29)(deflated 3%)</span><br><span class="line">adding: cache2.txt(<span class="keyword">in</span> = 37) (out= 35)(deflated 5%)</span><br><span class="line"></span><br><span class="line">$ hdfs dfs -put cachedir.jar samples/cachefile</span><br><span class="line"></span><br><span class="line">$ hdfs dfs -cat /user/me/samples/cachefile/input.txt</span><br><span class="line">cachedir.jar/cache.txt</span><br><span class="line">cachedir.jar/cache2.txt</span><br><span class="line"></span><br><span class="line">$ cat test_jar/cache.txt</span><br><span class="line">This is just the cache string</span><br><span class="line"></span><br><span class="line">$ cat test_jar/cache2.txt</span><br><span class="line">This is just the second cache string</span><br><span class="line"></span><br><span class="line">$ hdfs dfs -ls /user/me/samples/cachefile/out</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r-* 1 me supergroup        0 2013-11-14 17:00 /user/me/samples/cachefile/out/_SUCCESS</span><br><span class="line">-rw-r--r-* 1 me supergroup       69 2013-11-14 17:00 /user/me/samples/cachefile/out/part-00000</span><br><span class="line"></span><br><span class="line">$ hdfs dfs -cat /user/me/samples/cachefile/out/part-00000</span><br><span class="line">This is just the cache string</span><br><span class="line">This is just the second cache string</span><br></pre></td></tr></table></figure>

<h2 id="使用范例"><a href="#使用范例" class="headerlink" title="使用范例"></a>使用范例</h2><h3 id="Hadoop分区类"><a href="#Hadoop分区类" class="headerlink" title="Hadoop分区类"></a>Hadoop分区类</h3><p>Hadoop有个对很多应用程序非常有用的类<code>KeyFieldBasePartitioner</code>.通过该类我们可以使Map/Reduce框架通过key中的一些字段来对输出进行分区,而不需要使用整个key域.例如:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-streaming-2.9.2.jar \</span><br><span class="line">  -D stream.map.output.field.separator=. \</span><br><span class="line">  -D stream.num.map.output.key.fields=4 \</span><br><span class="line">  -D map.output.key.field.separator=. \</span><br><span class="line">  -D mapreduce.partition.keypartitioner.options=-k1,2 \</span><br><span class="line">  -D mapreduce.job.reduces=12 \</span><br><span class="line">  -input myInputDirs \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper /bin/cat \</span><br><span class="line">  -reducer /bin/cat \</span><br><span class="line">  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner</span><br></pre></td></tr></table></figure>

<p>上面例子里,<em>-D stream.map.output.field.separator=.</em> 和 <em>-D stream.num.map.output.key.fields=4</em> 用法含义已经在前面解释过.整个两个变量主要用来确定streaming中的key和value.</p>
<p>Map/Reduce作业中map阶段的输出包含四个字段并用<code>.</code>隔开. Map/Reduce框架通过 <code>-Dmapred.text.key.partitioner.option=k1,2</code> 选项确定使用key中前两个字段将map输出分区. <code>-Dmap.output.key.field.separator=.</code>指定输出数据字段间的分割符号.从而保证了key中前两个字段相同的数据将分配给同一个reducer任务(写入到容一个分区中).</p>
<p>这相当于将key分成4个字段,前两个字段为联合主键其余部分为次主键. 主键部分用于分区(确定写到哪个分区上),主键与次主键组合用于字典排序.通过下面例子可以诠释一下:</p>
<p>map输出数据的key部分</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">11.12.1.2</span><br><span class="line">11.14.2.3</span><br><span class="line">11.11.4.1</span><br><span class="line">11.12.1.1</span><br><span class="line">11.14.2.2</span><br></pre></td></tr></table></figure>

<p>写入到三个分区中(前两个字段用于分区键)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">11.11.4.1</span><br><span class="line">-----------</span><br><span class="line">11.12.1.2</span><br><span class="line">11.12.1.1</span><br><span class="line">-----------</span><br><span class="line">11.14.2.3</span><br><span class="line">11.14.2.2</span><br></pre></td></tr></table></figure>

<p>每个分区中使用整个全部字段进行排序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">11.11.4.1</span><br><span class="line">-----------</span><br><span class="line">11.12.1.1</span><br><span class="line">11.12.1.2</span><br><span class="line">-----------</span><br><span class="line">11.14.2.2</span><br><span class="line">11.14.2.3</span><br></pre></td></tr></table></figure>

<h3 id="Hadoop-Comparator-Class"><a href="#Hadoop-Comparator-Class" class="headerlink" title="Hadoop Comparator Class"></a>Hadoop Comparator Class</h3><p>Hadoop类库中,KeyFieldBasedComparator也是一个非常有用的类库.它实现了Unix/GNU sort特定的一个子集.例如</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-streaming-2.9.2.jar \</span><br><span class="line">  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \</span><br><span class="line">  -D stream.map.output.field.separator=. \</span><br><span class="line">  -D stream.num.map.output.key.fields=4 \</span><br><span class="line">  -D mapreduce.map.output.key.field.separator=. \</span><br><span class="line">  -D mapreduce.partition.keycomparator.options=-k2,2nr \</span><br><span class="line">  -D mapreduce.job.reduces=1 \</span><br><span class="line">  -input myInputDirs \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper /bin/cat \</span><br><span class="line">  -reducer /bin/cat</span><br></pre></td></tr></table></figure>

<p>如前面讲使用<code>.</code>分割字段等等. <code>-k2,2nr</code>表示以key中第二个字段作为排序字段, -n表示数字排序, -r表示逆序.如下所示,map的输出内容为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">11.12.1.2</span><br><span class="line">11.14.2.3</span><br><span class="line">11.11.4.1</span><br><span class="line">11.12.1.1</span><br><span class="line">11.14.2.2</span><br></pre></td></tr></table></figure>
<p>排序后的内容为(使用第二个字段排序,并且是将序[14,14,12,12,11])</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">11.14.2.3</span><br><span class="line">11.14.2.2</span><br><span class="line">11.12.1.2</span><br><span class="line">11.12.1.1</span><br><span class="line">11.11.4.1</span><br></pre></td></tr></table></figure>
<h3 id="Hadoop-Aggregate-Package"><a href="#Hadoop-Aggregate-Package" class="headerlink" title="Hadoop Aggregate Package"></a>Hadoop Aggregate Package</h3><pre><code>Hadoop有一个类库Aggregate. Aggregate提供一些特定reducer类,combiner类,以及一些简单的聚合操操作,如&quot;sum&quot;,&quot;max&quot;,&quot;min&quot;等等. 使用Aggregate我们可以定义一些mapper插件类用于生成相应的聚合选项.</code></pre><p>一个聚合例子(-reducer aggregate)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-streaming-2.9.2.jar \</span><br><span class="line">  -input myInputDirs \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper myAggregatorForKeyCount.py \</span><br><span class="line">  -reducer aggregate \</span><br><span class="line">  -file myAggregatorForKeyCount.py \</span><br></pre></td></tr></table></figure>
<p>myAggregatorForKeyCount.py代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateLongCountToken</span><span class="params">(id)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"LongValueSum:"</span> + id + <span class="string">"\t"</span> + <span class="string">"1"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv)</span>:</span></span><br><span class="line">    line = sys.stdin.readline();</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">while</span> line:</span><br><span class="line">            line = line&amp;<span class="comment">#91;:-1];</span></span><br><span class="line">            fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="keyword">print</span> generateLongCountToken(fields&amp;<span class="comment">#91;0]);</span></span><br><span class="line">            line = sys.stdin.readline();</span><br><span class="line">    <span class="keyword">except</span> <span class="string">"end of file"</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">     main(sys.argv)</span><br></pre></td></tr></table></figure>

<h3 id="Hadoop-Field-Selection-Class"><a href="#Hadoop-Field-Selection-Class" class="headerlink" title="Hadoop Field Selection Class"></a>Hadoop Field Selection Class</h3><p>Hadoop has a library class, FieldSelectionMapReduce, that effectively allows you to process text data like the unix “cut” utility. The map function defined in the class treats each input key/value pair as a list of fields. You can specify the field separator (the default is the tab character). You can select an arbitrary list of fields as the map output key, and an arbitrary list of fields as the map output value. Similarly, the reduce function defined in the class treats each input key/value pair as a list of fields. You can select an arbitrary list of fields as the reduce output key, and an arbitrary list of fields as the reduce output value. For example:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-streaming-2.9.2.jar \</span><br><span class="line">  -D mapreduce.map.output.key.field.separator&#x3D;. \</span><br><span class="line">  -D mapreduce.partition.keypartitioner.options&#x3D;-k1,2 \</span><br><span class="line">  -D mapreduce.fieldsel.data.field.separator&#x3D;. \</span><br><span class="line">  -D mapreduce.fieldsel.map.output.key.value.fields.spec&#x3D;6,5,1-3:0- \</span><br><span class="line">  -D mapreduce.fieldsel.reduce.output.key.value.fields.spec&#x3D;0-2:5- \</span><br><span class="line">  -D mapreduce.map.output.key.class&#x3D;org.apache.hadoop.io.Text \</span><br><span class="line">  -D mapreduce.job.reduces&#x3D;12 \</span><br><span class="line">  -input myInputDirs \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \</span><br><span class="line">  -reducer org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \</span><br><span class="line">  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner</span><br></pre></td></tr></table></figure>

<p>The option “-D mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0-” specifies key/value selection for the map outputs. Key selection spec and value selection spec are separated by “:”. In this case, the map output key will consist of fields 6, 5, 1, 2, and 3. The map output value will consist of all fields (0- means field 0 and all the subsequent fields).</p>
<p>The option “-D mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5-” specifies key/value selection for the reduce outputs. In this case, the reduce output key will consist of fields 0, 1, 2 (corresponding to the original fields 6, 5, 1). The reduce output value will consist of all fields starting from field 5 (corresponding to all the original fields).</p>
<h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ul>
<li>How do I use Hadoop Streaming to run an arbitrary set of (semi) independent tasks?</li>
<li>How do I process files, one per map?</li>
<li>应该使用多少个reducer</li>
<li>在脚本里设置的别名,-mapper结束后还能用么?</li>
<li>能使用unix 管道么?<br>…</li>
</ul>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/2019/04/14/2019-04-20-NSQ%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-%E5%BB%B6%E8%BF%9F%E6%B6%88%E6%81%AF/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    NSQ延迟消息投递
                
            </div>
        </a>
    
    
        <a href="/2019/03/10/2019-03-10-NSQ%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-nsqlookupd%E7%BB%84%E4%BB%B6/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">NSQ源码阅读-nsqlookupd组件</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            yindex &copy; 2020 
            <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten" target="_blank" rel="noopener">wikitten</a>
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


    </div>
</body>
</html>