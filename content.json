{"meta":{"title":"数据与安全技术研究","subtitle":"数据驱动安全","description":"","author":"yindex","url":"https://blog.yindex.org","root":"/"},"pages":[{"title":"About","date":"2020-02-27T17:17:23.651Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"about/index.html","permalink":"https://blog.yindex.org/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2020-02-27T17:17:23.651Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"categories/index.html","permalink":"https://blog.yindex.org/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-02-27T17:17:23.651Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"tags/index.html","permalink":"https://blog.yindex.org/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"分布式快照算法 Chandy-Lamport","slug":"分布式快照算法 Chandy-Lamport","date":"2020-02-15T16:12:31.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2020/02/15/分布式快照算法 Chandy-Lamport/","link":"","permalink":"https://blog.yindex.org/2020/02/15/%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%AE%97%E6%B3%95%20Chandy-Lamport/","excerpt":"","text":"IntroductionA snapshot algorithm is used to create a consistent snapshot of the global state of a distributed system. Due to the lack of globally shared memory and a global clock, the isn’t trivially possible. 1. OverviewChandy-Lamport algorithm is named by two author’s name. One of them, Lamport, the author of Paxos. The distributed snapshot algorithm described here came about when I visited Chandy, who was then at the University of Texas in Austin. He posed the problem to me over dinner, but we had both had too much wine to think about it right then. The next morning, in the shower, I came up with the solution. When I arrived at Chandy’s office, he was waiting for me with the same solution. I consider the algorithm to be a straightforward application of the basic ideas from Time, Clocks and the Ordering of Events in a Distributed System. 2. Global Snapshot3. Chandy-Lamport4. Experiment5. 总结References","categories":[{"name":"flink","slug":"flink","permalink":"https://blog.yindex.org/categories/flink/"}],"tags":[{"name":"分布式快照","slug":"分布式快照","permalink":"https://blog.yindex.org/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7/"},{"name":"flink","slug":"flink","permalink":"https://blog.yindex.org/tags/flink/"}]},{"title":"LVS后端GO-Web获取用户IP","slug":"go-lvs-toa-ipv4","date":"2019-07-10T16:12:31.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2019/07/10/go-lvs-toa-ipv4/","link":"","permalink":"https://blog.yindex.org/2019/07/10/go-lvs-toa-ipv4/","excerpt":"","text":"0x01. 问题最近写了个对外的ti接口,测试时发现获取到的用户ip都是lvs的ip. 咨询hulk同事LVS是FULLNAT模式,宿主机器需要安装toa.然而安装完toa后依旧没有解决问题.z在宿主机器上查看go-web监听和链接情况: 1netstat -pant | grep ti 发现go gin框架默认以tcp6监听服务，通过查询资料得知toa仅支持tcp4, 那么就需要修改下源代码让服务监听tcp4. 0x02. 查看gin源码想看看gin是如何进行tcp监听的，查看gin.go代码289行发现，gin框架是使用go原生http.ListenAndServe启动的web服务. 12345678func (engine *Engine) Run(addr ...string) (err error) &#123; defer func() &#123; debugPrintError(err) &#125;() address := resolveAddress(addr) debugPrint(\"Listening and serving HTTP on %s\\n\", address) err = http.ListenAndServe(address, engine) return&#125; 通过查看ListenAndServe代码得知，go的net/http就是以默认tcp6启动的http服务. 1234567891011121314func (srv *Server) ListenAndServe() error &#123; if srv.shuttingDown() &#123; return ErrServerClosed &#125; addr := srv.Addr if addr == \"\" &#123; addr = \":http\" &#125; ln, err := net.Listen(\"tcp\", addr) if err != nil &#123; return err &#125; return srv.Serve(tcpKeepAliveListener&#123;ln.(*net.TCPListener)&#125;)&#125; 因此想通过gin的某些魔法设置从而以tcp4监听的想法是不可能了，并且gin的issue中也说明了这一点: https://github.com/gin-gonic/gin/issues/667 0x03. 手动监听通过手动启动tcp监听，然后配置handler来实现，tcp4启动的web服务. 123456789101112131415161718type Tcp4Router struct &#123; Handler http.Handler listener net.Listener server *http.Server&#125;func (t *Tcp4Router)Run(addr string) (err error) &#123; t.server = &amp;http.Server&#123;Addr:addr, Handler:t.Handler&#125; t.listener, err = net.Listen(\"tcp4\", addr) if err != nil &#123; return &#125; err = t.server.Serve(t.listener) return&#125; 0x04. 最后 生命在于折腾，在于学习… 致谢学习https://blog.csdn.net/achejq/article/details/73733920https://cloud.tencent.com/document/product/1022/31524","categories":[{"name":"后端开发","slug":"后端开发","permalink":"https://blog.yindex.org/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"go","slug":"go","permalink":"https://blog.yindex.org/tags/go/"},{"name":"lvs","slug":"lvs","permalink":"https://blog.yindex.org/tags/lvs/"},{"name":"tcp6","slug":"tcp6","permalink":"https://blog.yindex.org/tags/tcp6/"}]},{"title":"NSQ代码阅读与分析之inFlightPqueue与inFlightPqueue","slug":"2019-04-20-NSQ-代码阅读与分析-inFlightPqueue和PriorityQueue","date":"2019-04-20T00:00:00.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2019/04/20/2019-04-20-NSQ-代码阅读与分析-inFlightPqueue和PriorityQueue/","link":"","permalink":"https://blog.yindex.org/2019/04/20/2019-04-20-NSQ-%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-inFlightPqueue%E5%92%8CPriorityQueue/","excerpt":"","text":"Ref https://nsq.io/clients/tcp_protocol_spec.html https://github.com/nsqio/nsq https://blog.csdn.net/skh2015java/article/details/83419493 https://blog.csdn.net/skh2015java/article/details/83416045","categories":[{"name":"中间件","slug":"中间件","permalink":"https://blog.yindex.org/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.yindex.org/tags/golang/"},{"name":"nsq","slug":"nsq","permalink":"https://blog.yindex.org/tags/nsq/"},{"name":"消息队列","slug":"消息队列","permalink":"https://blog.yindex.org/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"中间件","slug":"中间件","permalink":"https://blog.yindex.org/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"消息投递","slug":"消息投递","permalink":"https://blog.yindex.org/tags/%E6%B6%88%E6%81%AF%E6%8A%95%E9%80%92/"}]},{"title":"NSQ代码阅读与分析之TOPIC","slug":"2019-04-20-NSQ-代码阅读与分析-TOPIC","date":"2019-04-20T00:00:00.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2019/04/20/2019-04-20-NSQ-代码阅读与分析-TOPIC/","link":"","permalink":"https://blog.yindex.org/2019/04/20/2019-04-20-NSQ-%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-TOPIC/","excerpt":"","text":"NSQ的Topic类型在nsq/nsqd/topic.go中定义，该文件中实现了Topic工厂方法，Topc状态管理、Channel(此channel非彼channel，该channel是nsq消费者通道)管理、消息写入、消息推送、落盘等. Topic类型定义12345678910111213141516171819202122232425262728type Topic struct &#123; // 64bit atomic vars need to be first for proper alignment on 32bit platforms messageCount uint64 //消息总大小 messageBytes uint64 sync.RWMutex //topic 名称 name string //消费者通道映射表 channelMap map[string]*Channel //后端落盘队列: 临时topic设置了伪落盘 backend BackendQueue //消息内存队列（chan) memoryMsgChan chan *Message startChan chan int exitChan chan int channelUpdateChan chan int waitGroup util.WaitGroupWrapper exitFlag int32 idFactory *guidFactory ephemeral bool deleteCallback func(*Topic) deleter sync.Once paused int32 pauseChan chan int // 节点上下文,包含节点各种信息 ctx *context&#125; Topic类型中定义Topic所必须的消息内存存储队列通道(golang channel)、落盘队列、以及消费者消费通道和一起其他的状态管理参数等. Topic创建通过func NewTopic(topicName string, ctx *context, deleteCallback func(*Topic)) *Topic工厂方法创建Topic实例，代码如下 123456789101112131415161718192021222324252627282930313233343536373839func NewTopic(topicName string, ctx *context, deleteCallback func(*Topic)) *Topic &#123; t := &amp;Topic&#123; name: topicName, channelMap: make(map[string]*Channel), memoryMsgChan: make(chan *Message, ctx.nsqd.getOpts().MemQueueSize), startChan: make(chan int, 1), exitChan: make(chan int), channelUpdateChan: make(chan int), ctx: ctx, paused: 0, pauseChan: make(chan int), deleteCallback: deleteCallback, idFactory: NewGUIDFactory(ctx.nsqd.getOpts().ID), &#125; if strings.HasSuffix(topicName, \"#ephemeral\") &#123; t.ephemeral = true // 不干活的后端落盘 t.backend = newDummyBackendQueue() &#125; else &#123; dqLogf := func(level diskqueue.LogLevel, f string, args ...interface&#123;&#125;) &#123; opts := ctx.nsqd.getOpts() lg.Logf(opts.Logger, opts.LogLevel, lg.LogLevel(level), f, args...) &#125; t.backend = diskqueue.New( topicName, ctx.nsqd.getOpts().DataPath, ctx.nsqd.getOpts().MaxBytesPerFile, int32(minValidMsgLength), int32(ctx.nsqd.getOpts().MaxMsgSize)+minValidMsgLength, ctx.nsqd.getOpts().SyncEvery, ctx.nsqd.getOpts().SyncTimeout, dqLogf, ) &#125; t.waitGroup.Wrap(t.messagePump) t.ctx.nsqd.Notify(t) return t&#125; 首先直接创建了一个Topic对象、初始化默认值等.而后判断消息topic是否为临时topic确定落盘队列性质.若为临时topic则newDummyBackenQueue方法给Topic.backend配置伪落盘队列.通过查看该方法实现可知,newDummyBackenQueue创建的落盘队列无任何功能,提供的接口都是空操作.若非临时Topic,则创建磁盘落盘队列diskQueue. Topic创建完成随即启动messagePump, 该方法从磁盘或内存中竞争选择一条消息写入到订阅该消息的消费者通道中. 从messagePump方法注释中也可以看出,消息落盘后将不能保证消息消费顺序. 同时调用Notify方法将该Topic信息通知给对应的nsqlookupd. func (t *Topic) PutMessage(m *Message) error 方法PutMessage方法实现了消息写入(http/tcp的pub接口就是调用该方法实现).PutMessage方法调用put方法写入消息.put方法尝试向 topic内存通道(go channel)扇入消息,若此时通道已经写满,那么该case语句将不能被执行. 于是执行default分支将消息写入到backendQueue存储到磁盘中. 1234567891011121314151617181920212223242526272829303132333435func (t *Topic) PutMessage(m *Message) error &#123; t.RLock() defer t.RUnlock() if atomic.LoadInt32(&amp;t.exitFlag) == 1 &#123; return errors.New(\"exiting\") &#125; err := t.put(m) if err != nil &#123; return err &#125; //原子增加消息数量 atomic.AddUint64(&amp;t.messageCount, 1) // 原则增加消息总大小 atomic.AddUint64(&amp;t.messageBytes, uint64(len(m.Body))) return nil&#125;func (t *Topic) put(m *Message) error &#123; select &#123; //若通道已满则执行default分支 case t.memoryMsgChan &lt;- m: default: b := bufferPoolGet() err := writeMessageToBackend(b, m, t.backend) bufferPoolPut(b) t.ctx.nsqd.SetHealth(err) if err != nil &#123; t.ctx.nsqd.logf(LOG_ERROR, \"TOPIC(%s) ERROR: failed to write message to backend - %s\", t.name, err) return err &#125; &#125; return nil&#125; func (t *Topic) messagePump()方法 messagePump selects over the in-memory and backend queue andwrites messages to every channel for this topic 上述是messagePump方法的注释,可以看出messagePump方法主要功能为:从内存通道或磁盘中竞争选择消息写入到订阅通道中. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596func (t *Topic) messagePump() &#123; var msg *Message var buf []byte var err error var chans []*Channel var memoryMsgChan chan *Message var backendChan chan []byte // do not pass messages before Start(), but avoid blocking Pause() or GetChannel() for &#123; select &#123; case &lt;-t.channelUpdateChan: continue case &lt;-t.pauseChan: continue case &lt;-t.exitChan: goto exit case &lt;-t.startChan: &#125; break &#125; t.RLock() for _, c := range t.channelMap &#123; chans = append(chans, c) &#125; t.RUnlock() if len(chans) &gt; 0 &amp;&amp; !t.IsPaused() &#123; memoryMsgChan = t.memoryMsgChan backendChan = t.backend.ReadChan() &#125; // main message loop for &#123; select &#123; case msg = &lt;-memoryMsgChan: case buf = &lt;-backendChan: msg, err = decodeMessage(buf) if err != nil &#123; t.ctx.nsqd.logf(LOG_ERROR, \"failed to decode message - %s\", err) continue &#125; case &lt;-t.channelUpdateChan: chans = chans[:0] t.RLock() for _, c := range t.channelMap &#123; chans = append(chans, c) &#125; t.RUnlock() if len(chans) == 0 || t.IsPaused() &#123; memoryMsgChan = nil backendChan = nil &#125; else &#123; memoryMsgChan = t.memoryMsgChan backendChan = t.backend.ReadChan() &#125; continue case &lt;-t.pauseChan: if len(chans) == 0 || t.IsPaused() &#123; memoryMsgChan = nil backendChan = nil &#125; else &#123; memoryMsgChan = t.memoryMsgChan backendChan = t.backend.ReadChan() &#125; continue case &lt;-t.exitChan: goto exit &#125; for i, channel := range chans &#123; chanMsg := msg // copy the message because each channel // needs a unique instance but... // fastpath to avoid copy if its the first channel // (the topic already created the first copy) if i &gt; 0 &#123; chanMsg = NewMessage(msg.ID, msg.Body) chanMsg.Timestamp = msg.Timestamp chanMsg.deferred = msg.deferred &#125; if chanMsg.deferred != 0 &#123; channel.PutMessageDeferred(chanMsg, chanMsg.deferred) continue &#125; err := channel.PutMessage(chanMsg) if err != nil &#123; t.ctx.nsqd.logf(LOG_ERROR, \"TOPIC(%s) ERROR: failed to put msg(%s) to channel(%s) - %s\", t.name, msg.ID, channel.name, err) &#125; &#125; &#125;exit: t.ctx.nsqd.logf(LOG_INFO, \"TOPIC(%s): closing ... messagePump\", t.name)&#125; messagePump方法启动后,必须调用Start()方法: do not pass messages before Start(), but avoid blocking Pause() or GetChannel() messagePump进入主消息循环, 此处是一个select case操作,竞争执行,其中有5个竞争条件. msg = &lt;- memoryMsgChan buf = &lt;- backendChan &lt;- t.channelUpdateChan &lt;- t.pauseChan &lt;- t.exitChan 通过竞争条件看出,topic启动后若有新消费者接入,及&lt;- t.channelUpdateChan分支能够执行,该分支操作将更新消费者订阅通道. 若磁盘和内存通道中都存在消息,那么条件1和条件2将竞争执行(谁抢到算谁的),因此消息一旦落入磁盘将不在保证消费顺序. 消息写入订阅通道 在源码topic.go:314行, Topic遍历订阅通道,并将消息写入到消费者通道中. 123456789101112131415if i &gt; 0 &#123; chanMsg = NewMessage(msg.ID, msg.Body) chanMsg.Timestamp = msg.Timestamp chanMsg.deferred = msg.deferred&#125;if chanMsg.deferred != 0 &#123; channel.PutMessageDeferred(chanMsg, chanMsg.deferred) continue&#125;err := channel.PutMessage(chanMsg)if err != nil &#123; t.ctx.nsqd.logf(LOG_ERROR, \"TOPIC(%s) ERROR: failed to put msg(%s) to channel(%s) - %s\", t.name, msg.ID, channel.name, err)&#125; 通过消息Message类型中deferred类型判断该消息是否为延迟消息,若为延迟消息则写入消费者通道的延迟队列中.否则写入到消费者消费队列中. Ref https://nsq.io/clients/tcp_protocol_spec.html https://github.com/nsqio/nsq","categories":[{"name":"中间件","slug":"中间件","permalink":"https://blog.yindex.org/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.yindex.org/tags/golang/"},{"name":"nsq","slug":"nsq","permalink":"https://blog.yindex.org/tags/nsq/"},{"name":"消息队列","slug":"消息队列","permalink":"https://blog.yindex.org/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"中间件","slug":"中间件","permalink":"https://blog.yindex.org/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"topic","slug":"topic","permalink":"https://blog.yindex.org/tags/topic/"}]},{"title":"NSQ代码阅读与分析之Channel","slug":"2019-04-20-NSQ-代码阅读与分析-Channel","date":"2019-04-15T00:00:00.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2019/04/15/2019-04-20-NSQ-代码阅读与分析-Channel/","link":"","permalink":"https://blog.yindex.org/2019/04/15/2019-04-20-NSQ-%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-Channel/","excerpt":"","text":"此channel非彼channel,只是跟golang的channel同名而已,它是NSQ消费者订阅特定Topic的一种抽象.nsqd抽到生产者push的message后,会遍历当前topic下所有消费者channel并深度拷贝message下发到所有消费这channel中.并且同意个消费通道只会被投递一次.其中nsqd的deferred message在此实现.下图是nsq官网的一个消息流向图. channel创建于初始化12345678910111213141516171819202122232425262728293031323334353637383940414243444546func NewChannel(topicName string, channelName string, ctx *context, deleteCallback func(*Channel)) *Channel &#123; c := &amp;Channel&#123; topicName: topicName, name: channelName, memoryMsgChan: make(chan *Message, ctx.nsqd.getOpts().MemQueueSize), clients: make(map[int64]Consumer), deleteCallback: deleteCallback, ctx: ctx, &#125; if len(ctx.nsqd.getOpts().E2EProcessingLatencyPercentiles) &gt; 0 &#123; c.e2eProcessingLatencyStream = quantile.New( ctx.nsqd.getOpts().E2EProcessingLatencyWindowTime, ctx.nsqd.getOpts().E2EProcessingLatencyPercentiles, ) &#125; c.initPQ() if strings.HasSuffix(channelName, \"#ephemeral\") &#123; c.ephemeral = true c.backend = newDummyBackendQueue() &#125; else &#123; dqLogf := func(level diskqueue.LogLevel, f string, args ...interface&#123;&#125;) &#123; opts := ctx.nsqd.getOpts() lg.Logf(opts.Logger, opts.LogLevel, lg.LogLevel(level), f, args...) &#125; // backend names, for uniqueness, automatically include the topic... backendName := getBackendName(topicName, channelName) c.backend = diskqueue.New( backendName, ctx.nsqd.getOpts().DataPath, ctx.nsqd.getOpts().MaxBytesPerFile, int32(minValidMsgLength), int32(ctx.nsqd.getOpts().MaxMsgSize)+minValidMsgLength, ctx.nsqd.getOpts().SyncEvery, ctx.nsqd.getOpts().SyncTimeout, dqLogf, ) &#125; c.ctx.nsqd.Notify(c) return c&#125; NewChannel工厂方法首先根据topicName, channelName创建了一个Channel类型类型实例,并初始化memoryMsgchan(内存channel)和一个消费者map.然后调用initPQ方法初始化其他参数. 若当前channel为临时channel,则创建一个伪后端落盘队列.否则通过diskqueue.New方法创建一个后端落盘队列. 最后调用c.ctx.nsqd.Notify(c)将该channel通知给当前集群的nsqlookupd. ###channel退出与释放 有创建channel就有对应的删除channel方法.源码nsqd/channel.go:152处可以看到该方法被完全锁保护,因此channel退出方法不能并发执行. channel是消费者订阅通道,因此channel退出前需要关闭对应的消费者订阅连接.nsqd/channel.go:169位置通过读写锁保护,因此channel在退出时不允许新的消费着在创建连接. 最后将该channel中剩余的内容写入到磁盘. 写入消息PutMessagePutMessage通过调用put方法写入到内存或者磁盘中.其中put方法实现如下: 12345678910111213141516func (c *Channel) put(m *Message) error &#123; select &#123; case c.memoryMsgChan &lt;- m: default: b := bufferPoolGet() err := writeMessageToBackend(b, m, c.backend) bufferPoolPut(b) c.ctx.nsqd.SetHealth(err) if err != nil &#123; c.ctx.nsqd.logf(LOG_ERROR, \"CHANNEL(%s): failed to write message to backend - %s\", c.name, err) return err &#125; &#125; return nil&#125; 如果channel的内存通道未满则将消息写入到内存通道中(Channel.memoryMsgChan)否则执行默认分支写入到后端磁盘队列(如果是临时channel这部分操作将是无效操作). 若写磁盘出错会调用setHealth设置当前节点健康状态. 读/写延迟消息PutMessageDeferredPutMessageDeferred通过调用StartDeferredTimeout(msg, timeout)实现.其中timeout为该消息到期时间. StartDeferredTimeout方法实现如下: 12345678910func (c *Channel) StartDeferredTimeout(msg *Message, timeout time.Duration) error &#123; absTs := time.Now().Add(timeout).UnixNano() item := &amp;pqueue.Item&#123;Value: msg, Priority: absTs&#125; err := c.pushDeferredMessage(item) if err != nil &#123; return err &#125; c.addToDeferredPQ(item) return nil&#125; 首先计算出到期时间的时间戳,并将该事件戳设置到Item类型nsqd/channel.go:441的Priority字段.通过addToDeferredPQ方法nsqd/channel.go:446 将包含消延迟消息的Item添加到当前channel的deferredPQ字段. deferredPQ是一个pqueue.PriorityQueue类型,该类型是一个小根堆优先队列,也就是Priority值越小优先级越高所以时间戳越小越靠前. processDeferredQueue方法是处理延迟消息的消息循环,它的代码如下: 123456789101112131415161718192021222324252627282930func (c *Channel) processDeferredQueue(t int64) bool &#123; c.exitMutex.RLock() defer c.exitMutex.RUnlock() if c.Exiting() &#123; return false &#125; dirty := false for &#123; c.deferredMutex.Lock() item, _ := c.deferredPQ.PeekAndShift(t) c.deferredMutex.Unlock() if item == nil &#123; goto exit &#125; dirty = true msg := item.Value.(*Message) _, err := c.popDeferredMessage(msg.ID) if err != nil &#123; goto exit &#125; c.put(msg) &#125;exit: return dirty&#125; 循环体中,通过调用c.deferredPQ.PeekAndShift(t)扇出一条符合条件的消息.其中t为当前事件戳.那么怎么才算符合条件呢?查看PeekAndShift代码知道只有消息的优先级(事件戳)大于当前时间戳才算符合. 最后将扇出的消息投递回普通的消费者订阅通道(这块比较棒,代码复用不在重复发送逻辑). 12345678910111213func (pq *PriorityQueue) PeekAndShift(max int64) (*Item, int64) &#123; if pq.Len() == 0 &#123; return nil, 0 &#125; item := (*pq)[0] if item.Priority &gt; max &#123; return nil, item.Priority - max &#125; heap.Remove(pq, 0) return item, 0&#125; 为什么不是大于等于呢? ###最后 channel和topic的实现类似,并且给了我一种channel和topic可以再抽象一层做代码复用的感觉. 两者都是内存通道+后端落盘.都是磁盘消息与内存消息竞争执行处理. Ref https://nsq.io/clients/tcp_protocol_spec.html https://github.com/nsqio/nsq https://blog.csdn.net/skh2015java/article/details/83419493","categories":[{"name":"中间件","slug":"中间件","permalink":"https://blog.yindex.org/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.yindex.org/tags/golang/"},{"name":"nsq","slug":"nsq","permalink":"https://blog.yindex.org/tags/nsq/"},{"name":"消息队列","slug":"消息队列","permalink":"https://blog.yindex.org/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"中间件","slug":"中间件","permalink":"https://blog.yindex.org/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"Channel","slug":"Channel","permalink":"https://blog.yindex.org/tags/Channel/"}]},{"title":"NSQ延迟消息投递","slug":"2019-04-20-NSQ源码阅读与分析-延迟消息","date":"2019-04-14T00:00:00.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2019/04/14/2019-04-20-NSQ源码阅读与分析-延迟消息/","link":"","permalink":"https://blog.yindex.org/2019/04/14/2019-04-20-NSQ%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E4%B8%8E%E5%88%86%E6%9E%90-%E5%BB%B6%E8%BF%9F%E6%B6%88%E6%81%AF/","excerpt":"","text":"Deferred MessageNSQ support deferred message, We can publish a deferred message to a topic by DPUB.NSQ目前在TCP客户端呢支持独立接口延迟消息,http PUB接口作为参数选项支持.并且nsq版本需要大于等于0.3.6. 12345DPUB &lt;topic_name&gt; &lt;defer_time&gt;\\n[ 4-byte size in bytes ][ N-byte binary data ]&lt;topic_name&gt; - a valid string (optionally having #ephemeral suffix)&lt;defer_time&gt; - a string representation of integer D which defines the time for how long to defer where 0 &lt;= D &lt; max-requeue-timeout Success Response 1OK Error Response 1234E_INVALIDE_BAD_TOPICE_BAD_MESSAGEE_DPUB_FAILED Deferred Message实现Deferred Message produceDeferred Message 在TCP接口中提供, 那就从消息生产开始阅读. nsqd/protocl_v2.go中TCP路由实现如下: 123456789101112131415161718192021222324252627282930313233343536// Exec 可以理解为 tcp 的 routerfunc (p *protocolV2) Exec(client *clientV2, params [][]byte) ([]byte, error) &#123; if bytes.Equal(params[0], []byte(\"IDENTIFY\")) &#123; // client 注册 return p.IDENTIFY(client, params) &#125; err := enforceTLSPolicy(client, p, params[0]) if err != nil &#123; return nil, err &#125; switch &#123; case bytes.Equal(params[0], []byte(\"FIN\")): // 一条消息消费完毕 return p.FIN(client, params) case bytes.Equal(params[0], []byte(\"RDY\")): // consumer 当前能消费多少消息 return p.RDY(client, params) case bytes.Equal(params[0], []byte(\"REQ\")): // 重新发送一条消息 return p.REQ(client, params) case bytes.Equal(params[0], []byte(\"PUB\")): // producer 发布一条消息 return p.PUB(client, params) case bytes.Equal(params[0], []byte(\"MPUB\")): // producer 发布多条消息 return p.MPUB(client, params) case bytes.Equal(params[0], []byte(\"DPUB\")): // producer 延迟发布消息 return p.DPUB(client, params) case bytes.Equal(params[0], []byte(\"NOP\")): // 无操作 return p.NOP(client, params) case bytes.Equal(params[0], []byte(\"TOUCH\")): // 修改消息的 timeout 时间，但是不重新发送 return p.TOUCH(client, params) case bytes.Equal(params[0], []byte(\"SUB\")): // consumer 订阅 channel return p.SUB(client, params) case bytes.Equal(params[0], []byte(\"CLS\")): // client close return p.CLS(client, params) case bytes.Equal(params[0], []byte(\"AUTH\")): // client 鉴权 return p.AUTH(client, params) &#125; return nil, protocol.NewFatalClientErr(nil, \"E_INVALID\", fmt.Sprintf(\"invalid command %s\", params[0]))&#125; 其中bytes.Equal(params[0], []byte(“DPUB”)这个分支为延迟消息投递方式,根据NSQ TCP协议中第一个参数确定路由去向,当你参数为DPUB时生产延迟消息,调用protocolV2.DPUB读取生产者生产的内容. protocolV2.DPUB主要读取消息内容、TOPIC以及它特有的延迟时间. 延迟投递时间在协议参数的中第三个参数中: 1 timeoutDuration := time.Duration(timeoutMs) * time .Millisecond 从代码中可以看出,deferred message的单位时毫秒. 最后将延迟时间设置到Message类型的deferred字段上,并通过把该消息投递到topic中. 延迟消息与其他消息的实现基本相同,唯一的区别就是延迟消息设置了msg.deferred. 1234topic := p.ctx.nsqd.GetTopic(topicName)msg := NewMessage(topic.GenerateID(), messageBody)msg.deferred = timeoutDurationerr = topic.PutMessage(msg) Deferred Message Consume直接查看topic.go文件,消息投递从该文件Topic.messagePump开始, 在该方法中把后端消息内容平行复制到N个消费通道中.消息复制关键代码如下topic.go/Topic.messagePump方法: 123456789101112131415161718192021222324for i, channel := range chans &#123; chanMsg := msg // copy the message because each channel // needs a unique instance but... // fastpath to avoid copy if its the first channel // (the topic already created the first copy) // 这里 msg 实例要进行深拷贝，因为每个 channel 需要自己的实例 // 为了重发/延迟发送等 if i &gt; 0 &#123; chanMsg = NewMessage(msg.ID, msg.Body) chanMsg.Timestamp = msg.Timestamp chanMsg.deferred = msg.deferred &#125; if chanMsg.deferred != 0 &#123; channel.PutMessageDeferred(chanMsg, chanMsg.deferred) continue &#125; err := channel.PutMessage(chanMsg) if err != nil &#123; t.ctx.nsqd.logf(LOG_ERROR, \"TOPIC(%s) ERROR: failed to put msg(%s) to channel(%s) - %s\", t.name, msg.ID, channel.name, err) &#125; &#125; 从上述代码中可以看到当投递过来的消息(Message)的deferred字段不为0时, 则通过通道的PutMessageDeferred方法将消息放回延迟队列中.否则将消息投递至消费通道队列. 查看PutMessageDeferred方法实现方式继续跟踪延迟消息去向: 1234func (c *Channel) PutMessageDeferred(msg *Message, timeout time.Duration) &#123; atomic.AddUint64(&amp;c.messageCount, 1) c.StartDeferredTimeout(msg, timeout)&#125; StartDeferredTimeout实现: 123456789101112// StartDeferredTimeout 将一条消息写入 Deferred 队列func (c *Channel) StartDeferredTimeout(msg *Message, timeout time.Duration) error &#123; absTs := time.Now().Add(timeout).UnixNano() // 生成延迟发送的时间 item := &amp;pqueue.Item&#123;Value: msg, Priority: absTs&#125; err := c.pushDeferredMessage(item) if err != nil &#123; return err &#125; c.addToDeferredPQ(item) return nil&#125; StartDeferredTimeout方法执行了两个操作:pushDeferredMessage, addToDeferredPQ pushDeferredMessage方法将消息写入到延迟消息通道的Map类型字段上(可以去重复~) addToDeferredPQ方法将消息写入 DeferredPQ 这个优先级队列 我们查看channel.go发现, func (c *Channel) processDeferredQueue(t int64) bool函数, 发现在此处对存储延迟消息的优先队列进行了弹出操作. 123456789101112131415161718192021222324252627func (c *Channel) processDeferredQueue(t int64) bool &#123; c.exitMutex.RLock() defer c.exitMutex.RUnlock() if c.Exiting() &#123; return false &#125; dirty := false for &#123; c.deferredMutex.Lock() item, _ := c.deferredPQ.PeekAndShift(t) c.deferredMutex.Unlock() if item == nil &#123; goto exit &#125; dirty = true msg := item.Value.(*Message) _, err := c.popDeferredMessage(msg.ID) if err != nil &#123; goto exit &#125; c.put(msg) &#125;exit: return dirty&#125; 其中这一句item, _ := c.deferredPQ.PeekAndShift(t)进行了时间判断,看一下实现方式: 12345678910111213func (pq *PriorityQueue) PeekAndShift(max int64) (*Item, int64) &#123; if pq.Len() == 0 &#123; return nil, 0 &#125; item := (*pq)[0] if item.Priority &gt; max &#123; return nil, item.Priority - max &#125; heap.Remove(pq, 0) return item, 0&#125; PeekAndShift方法中根据消息的到期时间与传入的max做了比较,如果消息到期时间大于传入的max,则返回该item,否则返回nil, 根据调用链可以知道max值为当前时间. 再回到调PeekAndShift的processDeferredQueue方法中看到,如果PeekAndShift成功了,则进行pop操作,并将该消息put到非延迟队列.(反射时deferred参数未设置,则默认为0,所以投递后不会在回到延迟队列). Ref https://nsq.io/clients/tcp_protocol_spec.html https://github.com/nsqio/nsq","categories":[{"name":"中间件","slug":"中间件","permalink":"https://blog.yindex.org/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.yindex.org/tags/golang/"},{"name":"nsq","slug":"nsq","permalink":"https://blog.yindex.org/tags/nsq/"},{"name":"消息队列","slug":"消息队列","permalink":"https://blog.yindex.org/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"中间件","slug":"中间件","permalink":"https://blog.yindex.org/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"延迟投递","slug":"延迟投递","permalink":"https://blog.yindex.org/tags/%E5%BB%B6%E8%BF%9F%E6%8A%95%E9%80%92/"}]},{"title":"Hadoop Streaming手册[译]","slug":"2019-04-01-Hadoop Streaming[译]","date":"2019-04-01T00:00:00.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2019/04/01/2019-04-01-Hadoop Streaming[译]/","link":"","permalink":"https://blog.yindex.org/2019/04/01/2019-04-01-Hadoop%20Streaming[%E8%AF%91]/","excerpt":"","text":"概述hadoop streaming是hadoop发行版附带的功能. 该功能让我们可以使用任何程序或脚本作为mapper和reducer 来创建Map/Reduce作业.例如： 12345hadoop jar hadoop-streaming-2.9.2.jar \\ -input inputDirs \\ -output outputDirs \\ -mapper /bin/cat \\ -reducer /usr/bin/wc Streaming运行原理上述例子中，mapper和reducer都是执行程序，该执行程序从标准输入(stdin)逐行读取数据并将数据吐到标准输出(stdout)中.hadoop streaming创建Map/Reduce作业、提交到对应的集群中并监控作业运行进度. 指定一个可执行程序作为mapper并初始化成功后，每个mapper任务将启动该执行程序. 当mapper作业运行时，它讲输入数据转化成行并逐行写入到该执行程序进程的标准输入中.同时mapper作业面向行收集该执行程序进程的标准输出(以\\n作为分割符切分数据)并将每行数据转成成key/value格式，将转化后的kv数据作为mapper作业的输出. 默认配置下，以tab作为分割符，每行数据分割后第一部分作为key其余部分作为value.如果行数据中没有制表符那么整行数据作为key, value为空. 我们可以通过设置-inputformat命令行参数来个性化定制key value格式． 这是Map/Reduce框架与streaming mapper/reduce通信协议的基础. 用户可以通过设置 stream.non.zero.exit.is.failure参数（true|false）来确定streaming作业是以非零状态退出表示作业运行成功或失败．默认情况下,streaming 作业进程以非零状态退出表示任务失败. Streaming 命令行选项Streaming 支持hadoop通用命令行选项的同时也支持streaming命令行选项，下面展示了常规命令行选项的语法格式. 注意： 一定要把通用命令行选项设置放置在streaming命令行选项之前，否则命令行选项设置将无效. 1hadoop command [genericOptions] [streamingOptions] Ｈadoop streaming 命令行选项列表如下. 参数 必选 描述 -input 目录名或文件名 是 mapper作业输入数据位置 -output 目录名 是 reducer作业输出位置 -mapper 执行程序或Java类名 否 Mapper执行程序，若未设置则默认使用IdentityMapper -reducer 执行程序或Java类名 否 Reducer执行程序，若未设置则默认使用IdentityReducer -file 文件名 否 让mapper,reducer,combiner作业能够在对应节点上本地化读取该文件 -inputformat JavaClassName 否 设置的类必须返回 key(Text类型)/Values(Text类型)键值对．若未设置该选项则默认使用TextInputFormat -outputformat JavaClassName 否 同上 -partioner JavaClassName 否 该类确定输出数据数据写入到哪一个reduce分区 -combiner streaming命令或JavaClassName 否 -cmdenv name=value 否 给streaming作业传递环境变量 -inputreader 否 -verbose 否 verbose输出 -lazyOutput 否 -numReduceTasks 否 reducer作业数量(mapreduce.job.reduces) -mapdebug 否 map作业失败时调用的脚本 -reducedebug 否 reduce作业失败时调用的脚本 ###指定Java类作为Mapper/Reducer 我们可以提供一个Java类作为mapper或者reducer 123456hadoop jar hadoop-streaming-2.9.2.jar \\-input inputdirs \\-output outputdir \\-inputformat org.apache.hadoop.mapred.KeyValueTextInputFormat \\-mapper org.apache.hadoop.mapred.lib.IdentifyMapper \\-reducer /usr/bin/wc 我们可以通过设置stream.non.zero.exit.is.failure为false或true来确定是否以执行程序退出非零状态码来表示任务成功或失败. 默认程序非零退出状态表示任务失败. ###文件打包到作业提交 我们可以执行任意执行程序作为mapper或reducer.可执行程序并不需要预先保存在集群上;若未提前保存在集群节点中,则需要通过-file选项通知框架打包相应执行程序作为作业提交的一部分. 例如: 123456hadoop jar hadoop-streaming-2.9.2.jar \\-input inputdirs \\-output outputdir \\-mapper mapper.py \\-reducer /usr/bin/wc \\-file mapper.py 上如例子中指定了一个用户定义的python 可执行脚本作为mapper. -file mapper.py 将python可执行文件作为作业提交的一部分传输到集群.除了打包可执行文件,还可以打包其他辅助文件(比如字典文件,配置文件等)供mapper作业使用. 如下所示: 1234567hadoop jar hadoop-streaming-2.9.2.jar \\ -input myInputDirs \\ -output myOutputDir \\ -mapper myPythonScript.py \\ -reducer /usr/bin/wc \\ -file myPythonScript.py \\ -file myDictionary.txt ###为作业指定其他插件 跟其他普通Map/Reduce作业一样,我们可以指定其他插件到streaming作业. 1234-inputformat JavaClassName-outputformat JavaClassName-partitioner JavaClassName-combiner streamingCommand or JavaClassName 给输入格式指定的Java类必须返回Text 格式的键值对类型. 若没有特别指定输入格式处理类,那么TextInputFormat将作为默认处理类.由于TextInputFormat 返回LongWriteable类型key, 并且key不是输入数据的一部分,key会被丢弃.只有值会通过管道传递给streaming mapper. 为输出格式指定的类应该采用Text类型对键值对类型.如果没有指定输出格式处理类,TextOutputFormat作为默认处理类. ###设置环境变量 通过下述方式再streaming命令行中设置环境变量 1-cmdenv EXAMPLE_DIR=/home/example/dic/ 通用命令行选项Hadoop streaming支持streaming命令行选项也支持hadoop通用命令行选项.通用命令行选项设置方式如下 注意: 一定要把通用命令行选项设置放置在streaming命令行选项之前，否则命令行选项设置将无效. 1hadoop command [genericOptions] [streamingOptions] Streaming可以使用的hadoop命令行选项如下表所示: 参数 必须 描述 -conf configfile 否 指定特定的配置文件 -D 否 设置指定的属性(Dkey=value) -fs host:port or local 否 指定namenode -files 否 指定需要copy到集群的文件,多个文件用逗号分开 -libjars 否 指定需要copy到集群classpath的jar文件,多个文件用逗号分开 -archives 否 指定要上传到集群并解压到压缩文件,多个文件用逗号分开 ###使用-D选项设置配置变量 我们可以使用`-D=来设置额外的配置. 设置目录修改节点本地临时目录路径 1-D dfs.data.dir=/tmp 设置额外的本地临时目录 123-D mapred.local.dir=/tmp/local-D mapred.system.dir=/tmp/system-D mapred.temp.dir=/tmp/temp 注释: 作业配置参数跟多细节请参考mapred-default.xml ####设置仅运行Map任务作业 我们经常需要仅运行map函数来处理输入数据,仅仅需要把mapreduce.job.reduces设置为0即可达到此目的.设置为0后,Map/Reduce框架不再创建reduce任务,并且mapper任务等输出会作为作业的最终输出. 1-D mapreduce.job.reduces=0 ####设置指定数量的Reducers 设置指定数量的reducers,例如设置2个: 123456hadoop jar hadoo-streaming-2.9.2.jar \\-D mapreduce.job.reduces=2\\-input inputdir \\-output outputdir \\-mapper /bin/cat \\-reducer /usr/bin/wc 自定义输入行分割键值对的方式如前面讲到,Map/Reduce框架从mapper任务的标准输出读取行数据时,它会把行数据分割成呢键值对形式.默认配置下,到第一个制表符的数据作为键,其余的数据作为值(不包含制表符tab). 但是,我们可以修改默认配置.我们可以指定一个分割符号来替换制表符.并且也可以值指定分割后的哪一部分作为key.如下所示 1234567hadoop jar hadoop-streaming-2.9.2.jar \\ -D stream.map.output.field.separator=. \\ -D stream.num.map.output.key.fields=4 \\ -input myInputDirs \\ -output myOutputDir \\ -mapper /bin/cat \\ -reducer /bin/cat 上面的例子中,通过-D stream.map.output.field.separator=.设置.作为map输出的字段分割符.并且指定到第四个分割符的前缀部分作为key,其余部分作为值.如果一行数据中的风格符号.不够4个那么整行数据讲作为key,值部分为空. 类似的,我们可以使用-D stream.reduce.output.field.separator=SEP和 -D stream.num.reduce.output.fields=NUM 来确定reduce的输出数据中哪一部分作为key哪一部分作为value. 类似上述方式,可以通过stream.map.input.field.separato 和stream.reduce.input.field.separator来设置Map/Reduce作业的输入数据分割符号.默认设置是以制表符作为字段分割符号. 使用大文件和归档文件-files和-archives选项让我们可以在任务中使用普通文件和归档文件.它们的参数是指向上传到hdfs的文件或归档文件到URI.这些文件缓存在作业中.我们可以通过fs.default.name配置变量获取host和fs_port. 注意: -files和-archives选项是hadoop通用命令行选项,一定要把它们放置在hadoop streaming选项之前,否则设置无效. 让任务可访问文件-files选项在当前任务的工作目录创建指向本地文件副本的符号连接.下面例子中hadoop在当前工作目录中自动创建了名为testfile.txt的符号链接.链接指向了本地testfile.txt副本. 1-files hdfs://host:fs_port/usr/testfile.txt 也可以使用#来为-files 指定的文件创建一个不同名称的符号链接 1-files hdfs://host:fs_port/user/testfile.txt#cachefile.txt 如果有多个文件可以用逗号分开,如下所示 1-files /home/usr/testfile1.txt, /home/user/estfile2.txt 任务访问归档文件-archives选项让我们可以把本地jars文件复制到当前任务的工作目录并unjar(解压). 下面例子中,hadoop在任务当前工作目录中创建了一个名为testfile.jar的符号链接.这个符号链接指向了上传的jar文件解压后的目录. 1-archives hdfs://host:fs_port/user/testfile.jar 使用#符号为-archives上传文件、解压后的目录创建一个不同名字的符号链接. 1-archives hdfs://host:fs_port/user/testfile.tgz#tgzdir 下面例子中,input.txt包含两行数据,分别表示两个文件: cachedir.jar/cache.txt 和 cacheddir.jar/cache2.txt. cacheddir.jar是指向包含cache.txt和cache2.txt文件的归档目录的一个符号链接. 1234567891011121314151617181920212223242526272829303132333435363738hadoop jar hadoop-streaming-2.9.2.jar \\ -archives 'hdfs://hadoop-nn1.example.com/user/me/samples/cachefile/cachedir.jar' \\ -D mapreduce.job.maps=1 \\ -D mapreduce.job.reduces=1 \\ -D mapreduce.job.name=\"Experiment\" \\ -input \"/user/me/samples/cachefile/input.txt\" \\ -output \"/user/me/samples/cachefile/out\" \\ -mapper \"xargs cat\" \\ -reducer \"cat\"$ ls test_jar/cache.txt cache2.txt$ jar cvf cachedir.jar -C test_jar/ .added manifestadding: cache.txt(in = 30) (out= 29)(deflated 3%)adding: cache2.txt(in = 37) (out= 35)(deflated 5%)$ hdfs dfs -put cachedir.jar samples/cachefile$ hdfs dfs -cat /user/me/samples/cachefile/input.txtcachedir.jar/cache.txtcachedir.jar/cache2.txt$ cat test_jar/cache.txtThis is just the cache string$ cat test_jar/cache2.txtThis is just the second cache string$ hdfs dfs -ls /user/me/samples/cachefile/outFound 2 items-rw-r--r-* 1 me supergroup 0 2013-11-14 17:00 /user/me/samples/cachefile/out/_SUCCESS-rw-r--r-* 1 me supergroup 69 2013-11-14 17:00 /user/me/samples/cachefile/out/part-00000$ hdfs dfs -cat /user/me/samples/cachefile/out/part-00000This is just the cache stringThis is just the second cache string 使用范例Hadoop分区类Hadoop有个对很多应用程序非常有用的类KeyFieldBasePartitioner.通过该类我们可以使Map/Reduce框架通过key中的一些字段来对输出进行分区,而不需要使用整个key域.例如: 1234567891011hadoop jar hadoop-streaming-2.9.2.jar \\ -D stream.map.output.field.separator=. \\ -D stream.num.map.output.key.fields=4 \\ -D map.output.key.field.separator=. \\ -D mapreduce.partition.keypartitioner.options=-k1,2 \\ -D mapreduce.job.reduces=12 \\ -input myInputDirs \\ -output myOutputDir \\ -mapper /bin/cat \\ -reducer /bin/cat \\ -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner 上面例子里,-D stream.map.output.field.separator=. 和 -D stream.num.map.output.key.fields=4 用法含义已经在前面解释过.整个两个变量主要用来确定streaming中的key和value. Map/Reduce作业中map阶段的输出包含四个字段并用.隔开. Map/Reduce框架通过 -Dmapred.text.key.partitioner.option=k1,2 选项确定使用key中前两个字段将map输出分区. -Dmap.output.key.field.separator=.指定输出数据字段间的分割符号.从而保证了key中前两个字段相同的数据将分配给同一个reducer任务(写入到容一个分区中). 这相当于将key分成4个字段,前两个字段为联合主键其余部分为次主键. 主键部分用于分区(确定写到哪个分区上),主键与次主键组合用于字典排序.通过下面例子可以诠释一下: map输出数据的key部分 1234511.12.1.211.14.2.311.11.4.111.12.1.111.14.2.2 写入到三个分区中(前两个字段用于分区键) 123456711.11.4.1-----------11.12.1.211.12.1.1-----------11.14.2.311.14.2.2 每个分区中使用整个全部字段进行排序 123456711.11.4.1-----------11.12.1.111.12.1.2-----------11.14.2.211.14.2.3 Hadoop Comparator ClassHadoop类库中,KeyFieldBasedComparator也是一个非常有用的类库.它实现了Unix/GNU sort特定的一个子集.例如 1234567891011hadoop jar hadoop-streaming-2.9.2.jar \\ -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\ -D stream.map.output.field.separator=. \\ -D stream.num.map.output.key.fields=4 \\ -D mapreduce.map.output.key.field.separator=. \\ -D mapreduce.partition.keycomparator.options=-k2,2nr \\ -D mapreduce.job.reduces=1 \\ -input myInputDirs \\ -output myOutputDir \\ -mapper /bin/cat \\ -reducer /bin/cat 如前面讲使用.分割字段等等. -k2,2nr表示以key中第二个字段作为排序字段, -n表示数字排序, -r表示逆序.如下所示,map的输出内容为 1234511.12.1.211.14.2.311.11.4.111.12.1.111.14.2.2 排序后的内容为(使用第二个字段排序,并且是将序[14,14,12,12,11]) 1234511.14.2.311.14.2.211.12.1.211.12.1.111.11.4.1 Hadoop Aggregate PackageHadoop有一个类库Aggregate. Aggregate提供一些特定reducer类,combiner类,以及一些简单的聚合操操作,如&quot;sum&quot;,&quot;max&quot;,&quot;min&quot;等等. 使用Aggregate我们可以定义一些mapper插件类用于生成相应的聚合选项.一个聚合例子(-reducer aggregate) 123456hadoop jar hadoop-streaming-2.9.2.jar \\ -input myInputDirs \\ -output myOutputDir \\ -mapper myAggregatorForKeyCount.py \\ -reducer aggregate \\ -file myAggregatorForKeyCount.py \\ myAggregatorForKeyCount.py代码如下 12345678910111213141516171819#!/usr/bin/pythonimport sys;def generateLongCountToken(id): return \"LongValueSum:\" + id + \"\\t\" + \"1\"def main(argv): line = sys.stdin.readline(); try: while line: line = line&amp;#91;:-1]; fields = line.split(\"\\t\"); print generateLongCountToken(fields&amp;#91;0]); line = sys.stdin.readline(); except \"end of file\": return Noneif __name__ == \"__main__\": main(sys.argv) Hadoop Field Selection ClassHadoop has a library class, FieldSelectionMapReduce, that effectively allows you to process text data like the unix “cut” utility. The map function defined in the class treats each input key/value pair as a list of fields. You can specify the field separator (the default is the tab character). You can select an arbitrary list of fields as the map output key, and an arbitrary list of fields as the map output value. Similarly, the reduce function defined in the class treats each input key/value pair as a list of fields. You can select an arbitrary list of fields as the reduce output key, and an arbitrary list of fields as the reduce output value. For example: 12345678910111213hadoop jar hadoop-streaming-2.9.2.jar \\ -D mapreduce.map.output.key.field.separator&#x3D;. \\ -D mapreduce.partition.keypartitioner.options&#x3D;-k1,2 \\ -D mapreduce.fieldsel.data.field.separator&#x3D;. \\ -D mapreduce.fieldsel.map.output.key.value.fields.spec&#x3D;6,5,1-3:0- \\ -D mapreduce.fieldsel.reduce.output.key.value.fields.spec&#x3D;0-2:5- \\ -D mapreduce.map.output.key.class&#x3D;org.apache.hadoop.io.Text \\ -D mapreduce.job.reduces&#x3D;12 \\ -input myInputDirs \\ -output myOutputDir \\ -mapper org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \\ -reducer org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \\ -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner The option “-D mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0-” specifies key/value selection for the map outputs. Key selection spec and value selection spec are separated by “:”. In this case, the map output key will consist of fields 6, 5, 1, 2, and 3. The map output value will consist of all fields (0- means field 0 and all the subsequent fields). The option “-D mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5-” specifies key/value selection for the reduce outputs. In this case, the reduce output key will consist of fields 0, 1, 2 (corresponding to the original fields 6, 5, 1). The reduce output value will consist of all fields starting from field 5 (corresponding to all the original fields). FAQ How do I use Hadoop Streaming to run an arbitrary set of (semi) independent tasks? How do I process files, one per map? 应该使用多少个reducer 在脚本里设置的别名,-mapper结束后还能用么? 能使用unix 管道么?…","categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://blog.yindex.org/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://blog.yindex.org/tags/hadoop/"},{"name":"streaming","slug":"streaming","permalink":"https://blog.yindex.org/tags/streaming/"}]},{"title":"NSQ源码阅读-nsqlookupd组件","slug":"2019-03-10-NSQ源码阅读-nsqlookupd组件","date":"2019-03-10T00:00:00.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2019/03/10/2019-03-10-NSQ源码阅读-nsqlookupd组件/","link":"","permalink":"https://blog.yindex.org/2019/03/10/2019-03-10-NSQ%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-nsqlookupd%E7%BB%84%E4%BB%B6/","excerpt":"","text":"NSQ包含三大组件nsqd、nsqlookup、nsqadmin. nsqlookupd 是守护进程负责管理拓扑信息,客户端通过查询 nsqlookupd 来发现指定topic的生产者，并且 nsqd 节点广播topic和channel信息.它包含两个接口：TCP 接口:处理nsqd广播。HTTP 接口:客户端用它来发现和管理服务。 一、NSQLookupd类型nsqlookupd.go为nsqlookup组件的入口文件,它主要包含一个NSQLookupd结构类型和该类型的工厂方法func New(*Options)*NSQLookupd. NSQLookupd类型属性定义如下. 12345678type NSQLookupd struct &#123; sync.RWMutex opts *Options tcpListener net.Listener httpListener net.Listener waitGroup util.WaitGroupWrapper DB *RegistrationDB&#125; sync.RWMutex 读写锁，主要用来更新DB中的Registration，Topic, Channels. opts NSQLookup相关配置. tcpListener 用于nsqd与nsqlookupd通信，默认监听4160端口. httpListener 用于nsqlookupd与nsqadmin通信默认监听4161端口. waitGroup 装饰器模式，wrap了一层sync.WaitGroup，用于系统退出时同步tcpListener、httpListener结束. DB 存储当前注册的Registration， Topic， Chanel，用于nsqd的数据管理. NSQLookupd类型绑定的方法列表如下. 12345- func (n *NSQLookupd) logf(level lg.LogLevel, f string, args ...interface&#123;&#125;)- func Main() error- func RealTCPAddr() *net.TCPAddr- func RealHTTPAddr() *net.TCPAddr- func Exit() logf方法 内部调用了internal日志方法,用于NSQLookupd上下文写日志. Main方法主要负责启动、停止,维护tcp http server等操作. RealTCPAddr, RealHTTPAddr获取当前服务地址. Exit() 同步关闭tcp http服务. 二、opts 成员opts变量主要存储NSQLookup相关配置信息，类型定义如下: 123456789101112type Options struct &#123; LogLevel string `flag:\"log-level\"` LogPrefix string `flag:\"log-prefix\"` Verbose bool `flag:\"verbose\"` Logger Logger logLevel lg.LogLevel TCPAddress string `flag:\"tcp-address\"` HTTPAddress string `flag:\"http-address\"` BroadcastAddress string `flag:\"broadcast-address\"` InactiveProducerTimeout time.Duration `flag:\"inactive-producer-timeout\"` TombstoneLifetime time.Duration `flag:\"tombstone-lifetime\"` &#125; LogLevel, LogPrefix定义了日志级别、日志前缀，Logger定义具体的日志类型，TCPAddress，HTTPAddress配置http，tcp相关服务监听地址.BroadcastAddress定义了广播地址. InactiveProducerTimeout 定义producer失活时间阈值，当超过InactiveProducerTimeout没有收到producer心跳时，则认为当前producer不在活跃. TombstoneLifetime避免发生竞争，当一个nsqd不再产生一个特定的toipc, 需要去掉这个toipc，这个时候，试图尝试删除topic信息与新的消费者已经发现这个主题的节点，重连, 会更新nsqlookup产生竞争. 三、waitGroup 成员waitGroup成员是util.WaitGroupWrapper类型,内部组合了sync.WaitGroup类型并绑定Wrap装饰方.用以系统退出时同步关闭tcpServer和httpServer服务进行安全退出. 在nslookupd.go的入口方法中，装箱tcpserver和httpserver服务nsqlookupd/nsqlookupd.go:64, 在nslookupd退出时调用Exit()方法nsqlookupd/nsqlookupd.go:87, 它会等待所有被wrap的方法都执行结束后（调用了wg.done后）进行安全退出. 四、httpListener成员httpListener成员类型定义在http.go中， 该文件定义httpServer的同时定义了node变量和一个httpServer的工厂方法.httpServer类型包含一个Context上下文类型和一个路由请求处理handler(nsq使用了julienschmidt/httprouter路由，号称最快路由). httpServer提供的api接口如下列表，同时它在注册路由时也使用了装饰器http_api.Decorate,装饰了log，http_api等. 同时httpServer提供了大量的DEBUG接口用于系统调试. 123456789101112131415161718192021router.Handle(\"GET\", \"/ping\", http_api.Decorate(s.pingHandler, log, http_api.PlainText))router.Handle(\"GET\", \"/info\", http_api.Decorate(s.doInfo, log, http_api.V1))router.Handle(\"GET\", \"/debug\", http_api.Decorate(s.doDebug, log, http_api.V1))router.Handle(\"GET\", \"/lookup\", http_api.Decorate(s.doLookup, log, http_api.V1))router.Handle(\"GET\", \"/topics\", http_api.Decorate(s.doTopics, log, http_api.V1))router.Handle(\"GET\", \"/channels\", http_api.Decorate(s.doChannels, log, http_api.V1))router.Handle(\"GET\", \"/nodes\", http_api.Decorate(s.doNodes, log, http_api.V1))router.Handle(\"POST\", \"/topic/create\", http_api.Decorate(s.doCreateTopic, log, http_api.V1))router.Handle(\"POST\", \"/topic/delete\", http_api.Decorate(s.doDeleteTopic, log, http_api.V1))router.Handle(\"POST\", \"/channel/create\", http_api.Decorate(s.doCreateChannel, log, http_api.V1))router.Handle(\"POST\", \"/channel/delete\", http_api.Decorate(s.doDeleteChannel, log, http_api.V1))router.Handle(\"POST\", \"/topic/tombstone\", http_api.Decorate(s.doTombstoneTopicProducer, log, http_api.V1))router.HandlerFunc(\"GET\", \"/debug/pprof\", pprof.Index)router.HandlerFunc(\"GET\", \"/debug/pprof/cmdline\", pprof.Cmdline)router.HandlerFunc(\"GET\", \"/debug/pprof/symbol\", pprof.Symbol)router.HandlerFunc(\"POST\", \"/debug/pprof/symbol\", pprof.Symbol)router.HandlerFunc(\"GET\", \"/debug/pprof/profile\", pprof.Profile)router.Handler(\"GET\", \"/debug/pprof/heap\", pprof.Handler(\"heap\"))router.Handler(\"GET\", \"/debug/pprof/goroutine\", pprof.Handler(\"goroutine\"))router.Handler(\"GET\", \"/debug/pprof/block\", pprof.Handler(\"block\"))router.Handler(\"GET\", \"/debug/pprof/threadcreate\", pprof.Handler(\"threadcreate\")) API接口比较多，功能也都顾名思义比较直接.这儿重点记录几个典型接口实现. func(s *httpServer) doCreateTopic创建topic接口; 内部实现主要包含了topic参数获取，topic检测，日志记录以及最重要的topic检测. Topic合法检测主要检测了起长度限(internal/protocol/names.go:22)制和一个正则检验(internal/protocol/names.go:8). Topic创建通过nsqlookupd的db成员中的AddRegistration方法实现,AddRegistration在实现注册时使用读写锁进行并发资源保护. 代码实现如下: 12345678func (r *RegistrationDB) AddRegistration(k Registration) &#123; r.Lock() defer r.Unlock() _, ok := r.registrationMap[k] if !ok &#123; r.registrationMap[k] = make(map[string]*Producer) &#125;&#125; func (s *httpServer) doLookup查询指定Topic的nsqd节点接口(nsqlookupd/http.go:105)，函数内部主要调用了DB变量绑定的方法，从而实现查询功能. httpServer提供的api接口方法主要在做与用户交互的一些功能，关于数据的管理都有后端db来提供实现.这也体现了mvc的思路吧算是. 五、tcpListener 成员tcpServer类型定义在tcp.go(nsqlookupd/tcp.go:10)中，函数内部包含一个NSQLookupd的Context变量，以及一个处理请求的方法func Handle(net.Conn). Handle方法主要完成两个操作. 客户端版本号识别调用io.ReadFull读取客户端请求中的版本号，根据该函数描述ReadFull reads exactly len(buf) bytes from r into buf. ，精确读取客户端请求前4字节数据作为客户端版本号. 当前代码版本仅支持版本号为V1的客户端, 若客户端合法则创建LookupProtocolV1类型并由该类型负责处理消息循环.其他版本则返回E_BAD_PROTOCOL IOLoop消息循环服务端首先对客户端请求进行预处理，以换行符号‘\\n’ 作为请求间的切分标志，并且针对每次请求以空格“ ”分割请求内容作为参数列表. 将请求参数列表和链接对象传给LookupProtocolV1.exec()处理. 根据客户端(nsqd)请求内容的第一个参数，确定请求功能类型. IOLoop通过Exec支持四种操作: PING, IDENTIFY, REGISTER UNREGISTER. PING 心跳包，nsqd存活检测, 并更新nsqd lastUpdate(原子操作) IDENTIFY 主要负责nsqd 连接tcpServer时候发送的注册包，根据nsqd发送的body内容实例化一个PeerInfo用来存储nsqd的meta信息，并讲PeerInfo存储到producer map中. 12345678peerInfo := PeerInfo&#123;id: client.RemoteAddr().String()&#125;err = json.Unmarshal(body, &amp;peerInfo)if err != nil &#123; return nil, protocol.NewFatalClientErr(err, \"E_BAD_BODY\", \"IDENTIFY failed to decode JSON body\")&#125;peerInfo.RemoteAddress = client.RemoteAddr().String()...atomic.StoreInt64(&amp;peerInfo.lastUpdate, time.Now().UnixNano()) REGISTER 获取topic channel构造Registration，存储到DB中 1234567if p.ctx.nsqlookupd.DB.AddProducer(key, &amp;Producer&#123;peerInfo: client.peerInfo&#125;)&#123; p.ctx.nsqlookupd.logf(LOG_INFO, \"DB: client(%s) REGISTER category:%s key:%s subkey:%s\",client, \"channel\", topic, channel)&#125;if p.ctx.nsqlookupd.DB.AddProducer(key, &amp;Producer&#123;peerInfo: client.peerInfo&#125;)&#123; p.ctx.nsqlookupd.logf(LOG_INFO, \"DB: client(%s) REGISTER category:%s key:%s subkey:%s\",client, \"topic\", topic, \"\")&#125; UNREGISTER 顾名思义为REGISTER逆操作. 六、DB成员DB成员类型定义在nsqlookupd/registration_db.go，该文件中定义以下7中类型，该文件内实现了nsqlookupd的核心业务模型逻辑，tcpServer和httpServer中对外提供的接口都是由该部分提供具体实现.应该算是模型层吧.该部分提供底层操作逻辑供tcpserver&amp;httpserver调用，因此阅读类型结构后根据tcpserver&amp;httpserver作为线索理清楚DB绑定方法. DB成员类型依赖关系图 DB成员提供的服务创建Topic在RegistrationDB中添加一个Registration key和默认的ProducerMap 并返回false.若已经存在则返回true 删除Topic先根据当前topic检索出该topic上绑定的channels，并删除所有的相关channel（channel也存储在registrationMap中）然后删除该topic相关信息(Producer， PeerInfo…)ps: channel 和 topic都存储在registrationMap中，由key:Registration{Category, Key, SubKey}来区分存储的数据类别. 屏蔽Topicnsqlookupd/http.go:doTombstoneTopicProducer, 屏蔽制定node的一个topic. 在http请求中获取要屏蔽的node和topic,并根据topicName检索出相应的Producer，根据请求中的node屏蔽Producer中对应的producer. 虽然go是值传递，for循环时是临时对象，但是根据Producers类型定义可以知道该类型中存储的是指针类型，所以再执行p.Tombstone()时是在具体地址上执行. 1type Producers []*Producer 创建Channel创建channel步骤与创建topic相同,都是调用AddRegistration，只是在创建channel后调用了一次创建该channel的topic（不会重复创建） 删除ChannelChannel删除比较直接，根据请求中的Channel值，检索出相关Registration然后删除. Lookup功能根据传入的topic值，检索判断是否存在对应的Registration.检索出相应的producer&amp;channels并返回. nsqd连接nsqlookupd [IDENTIFY] 解析出请求中peerInfo 原子操作更新peerInfo.lastUpdate 注册到Producer …… 参考链接 [1]nsq源码阅读 [2]go简单工厂模式 [3]工厂方法:维基百科 [4]golang 装饰模式 [5]Decorator pattern","categories":[{"name":"中间件","slug":"中间件","permalink":"https://blog.yindex.org/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.yindex.org/tags/golang/"},{"name":"nsq","slug":"nsq","permalink":"https://blog.yindex.org/tags/nsq/"},{"name":"消息队列","slug":"消息队列","permalink":"https://blog.yindex.org/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"nsqlookup","slug":"nsqlookup","permalink":"https://blog.yindex.org/tags/nsqlookup/"},{"name":"http","slug":"http","permalink":"https://blog.yindex.org/tags/http/"},{"name":"装饰器模式","slug":"装饰器模式","permalink":"https://blog.yindex.org/tags/%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/"}]},{"title":"交叉编译nginx-1.14.2 for hisi-arm-linux","slug":"2018-12-31-交叉编译nginx-1.14.2 for hisi-arm-linux","date":"2018-12-28T00:32:24.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2018/12/28/2018-12-31-交叉编译nginx-1.14.2 for hisi-arm-linux/","link":"","permalink":"https://blog.yindex.org/2018/12/28/2018-12-31-%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91nginx-1.14.2%20for%20hisi-arm-linux/","excerpt":"","text":"根据别人意见工程架构采用nginx+cgi的架构，当然工程中其他部分也对nginx有所依赖。所以需要把nginx移植到到hisi-arm-linux上。由于nginx自身不具备交叉编译支持，所以需要修改一些配置。 二、交叉编译nginx交叉编译环境: ubuntu 18.10 依赖： nginx-1.14.2 pcre-8.42， 仅编译必备选项，其他依赖不进行交叉编译支持（比如ssl） 工具链：aarch64-himix100-linux-gcc configure 配置 ./configure \\ --prefix=/root/nfs/nginx/ \\ --with-cc=aarch64-himix100-linux-gcc \\ --with-cpp=aarch64-himix100-linux-g++ \\ --with-pcre=/home/xxx/nginx/pcre-8.42 \\ --without-http_gzip_module \\ --without-http_upstream_zone_module \\ --sbin-path=/root/nfs/nginx/nginx \\ --modules-path=/root/nfs/nginx/modules \\ --conf-path=/root/nfs/nginx/nginx.conf \\ --error-log-path=/root/nfs/nginx/logs/error.log \\ --http-log-path=/root/nfs/nginx/logs/access.log \\ --pid-path=/root/nfs/nginx/run/nginx.pid \\ --lock-path=/root/nfs/nginx/run/subsys/nginx.lock \\prefix 安装目录(nfs目录是挂载的网路磁盘) with-cc with-cpp交叉编译工具 with-pcre 是pcre源码路径，不需要单独编译 修改 auto/cc/name if [ &quot;$NGX_PLATFORM&quot; != win32 ]; then ngx_feature=&quot;C compiler&quot; ngx_feature_name= # ngx_feature_run=yes ngx_feature_run=no ngx_feature_incs= ngx_feature_path= ngx_feature_libs= ngx_feature_test= . auto/feature if [ $ngx_found = no ]; then echo echo $0: error: C compiler $CC is not found echo exit 1 fi fi修改 auto/types/sizeof ngx_test=&quot;$CC $CC_TEST_FLAGS $CC_AUX_FLAGS \\ ngx_test=&quot;gcc $CC_TEST_FLAGS $CC_AUX_FLAG \\ if [ -x $NGX_AUTOTEST ]; then ngx_size=`$NGX_AUTOTEST` ngx_size=4修改auto/options 140 USE_PCRE=NO 141 PCRE=NONE 142 PCRE_OPT= 143 #PCRE_CONF_OPT= 144 PCRE_CONF_OPT=--host=arm-linux 145 PCRE_JIT=NO修改objs/ngx_auto_config.h， 添加如下语句 #ifndef NGX_SYS_NERR #define NGX_SYS_NERR 132 #endif #ifndef NGX_HAVE_SYSVSHM #define NGX_HAVE_SYSVSHM 1 #endif 然后执行make 结束语 环境搭建完，这才完成1%的工作…","categories":[{"name":"nginx","slug":"nginx","permalink":"https://blog.yindex.org/categories/nginx/"}],"tags":[{"name":"交叉编译","slug":"交叉编译","permalink":"https://blog.yindex.org/tags/%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/"},{"name":"海思","slug":"海思","permalink":"https://blog.yindex.org/tags/%E6%B5%B7%E6%80%9D/"},{"name":"arm","slug":"arm","permalink":"https://blog.yindex.org/tags/arm/"},{"name":"nginx","slug":"nginx","permalink":"https://blog.yindex.org/tags/nginx/"}]},{"title":"交叉编译 lua for hisi-arm-linux","slug":"2018-12-28-交叉编译 lua for hisi-arm-linux","date":"2018-12-28T00:32:24.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2018/12/28/2018-12-28-交叉编译 lua for hisi-arm-linux/","link":"","permalink":"https://blog.yindex.org/2018/12/28/2018-12-28-%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%20lua%20for%20hisi-arm-linux/","excerpt":"","text":"最近被借到其他部门做开发, 由于不可抗力因素，项目宿主机器由x64 Ubuntu Server转移到某arm 裸kernel上。为降低机器负载、统一技术栈和其他不可抗力因素后端调度器用C++11重构原有JAVA工程，由C++ with lua实现决策支持引擎。 二、交叉编译lua5.3（ubunt18.10）下载安装交叉编译工具(一般没什么问题，如有问题查看对应交叉编译工具文档) cd aarch64-himix100-linux sudo bash aarch64-himix100-linux.install​ 下载lua5.3源代码,如官网描述，命令行执行下载解压缩 curl -R -O http://www.lua.org/ftp/lua-5.3.5.tar.gz tar zxf lua-5.3.5.tar.gz cd lua-5.3.5/srcLua交互式环境中具备查阅历史命令功能，该部分功能由readline实现, 我们在arm仅作为C++扩展脚本使用，因此不需readline.h。 修改源代码屏蔽该库(不然还得交叉编译readline), 我这儿是添加了一个宏用来交叉编译时去掉readline.h, 在lauconf.h 60行添加如下代码 #if defined(LUA_USE_ARM) #define LUA_USE_POSIX #define LUA_USE_DLOPEN /* needs an extra library: -ldl */ // 去掉下面这句 /*#define LUA_USE_READLINE*/ /* needs some extra libraries */ #endif修改src/Makefile, 在108（其他位置也行）行添加如下代码, -DLUA_USE_ARM表示lauconf.h这一段启动. arm: $(MAKE) $(ALL) SYSCFLAGS=&quot;-DLUA_USE_ARM&quot; SYSLIBS=&quot;-Wl,-E -ldl&quot;修改Makefile第9行，将gcc替换为hisi编译工具 #CC= gcc -std=gnu99 修改为 CC= aarch64-himix100-linux-gcc -std=gnu99现在就可以进行编译了, 直接在src目录下执行命令. make arm如果出现loadlocale.c相关错误,添加环境变量后重新编译 export LC_ALL=C根据根目录下Makefile可得，抽取部分核心文件:*.h头文件和liblua.a静态链接库 42 TO_BIN= lua luac 43 TO_INC= lua.h luaconf.h lualib.h lauxlib.h lua.hpp 44 TO_LIB= liblua.a 45 TO_MAN= lua.1 luac.1现在Lua交叉编译完成，可以把上述文件直接copy到hisi-arm-linux上运行使用。 三、C++ with lua test编写C++&amp;&amp;lua测试代码, 并使用交叉编译工具编译后传到hisi-arm-linux上运行. #include &lt;iostream&gt; #include &lt;string.h&gt; extern &quot;C&quot; { #include &quot;lua-5.3.5/src/lua.h&quot; #include &quot;lua-5.3.5/src/lauxlib.h&quot; #include &quot;lua-5.3.5/src/lualib.h&quot; } int main() { int a, b; while (std::cin &gt;&gt; a &gt;&gt; b) { lua_State *L = luaL_newstate(); luaopen_base(L); luaL_openlibs(L); luaL_dofile(L, &quot;event.lua&quot;); lua_getglobal(L, &quot;event&quot;); lua_pushnumber(L, a); lua_pushnumber(L, b); lua_call(L, 2, 0); } return 0; }​​​ aarch64-himix100-linux-g++ lua.cpp -L lua-5.3.5/src -static -llua -ldl -lm -o lua_ext​ -lm是链接math库（lua使用） -L指定链接库位置（交叉编译的arm平台lua库） lua代码 function event(a, b) if a + b &lt; 10 then print(&quot;welcome to C++ with lua regular&quot;) else print(&quot;welcome to C++ with lua regular&quot;) end; print(&quot;a + b&quot;, a + b) print(&quot;a * b&quot;, a * b) print(&quot;a ^ b&quot;, a ^ b) print(&quot;a / b&quot;, a / b) end;结束语作为C++忠实粉丝，强烈抵制某些场景上直接上C++这种重炮, 改换语言能直接提高30%性能的都是自身设计问题. lua: 我是一直小小小小鸟～ ref http://www.lua.org/download.html https://blog.csdn.net/themagickeyjianan/article/details/75301960 https://www.cnblogs.com/wajika/p/6592659.html","categories":[{"name":"lua","slug":"lua","permalink":"https://blog.yindex.org/categories/lua/"}],"tags":[{"name":"交叉编译","slug":"交叉编译","permalink":"https://blog.yindex.org/tags/%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/"},{"name":"lua","slug":"lua","permalink":"https://blog.yindex.org/tags/lua/"},{"name":"海思","slug":"海思","permalink":"https://blog.yindex.org/tags/%E6%B5%B7%E6%80%9D/"},{"name":"arm","slug":"arm","permalink":"https://blog.yindex.org/tags/arm/"}]},{"title":"基于CNN的MNIST手写识别尝试","slug":"基于cnn进行mnist手写识别","date":"2017-07-05T10:01:09.000Z","updated":"2020-02-27T17:17:23.651Z","comments":true,"path":"2017/07/05/基于cnn进行mnist手写识别/","link":"","permalink":"https://blog.yindex.org/2017/07/05/%E5%9F%BA%E4%BA%8Ecnn%E8%BF%9B%E8%A1%8Cmnist%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB/","excerpt":"","text":"基于CNN的MNIST手写识别尝试0x01 前言MNIST手写识别基本上是深度学习领域的Hello world了，本文主要是使用tensorflow炮筒以下流程，随便添加了几个网络层进行手写识别. 0x02 CNN网络结构配置0x0201 数据预处理因为mnist输入数据是一个784维的向量( $28\\times28$ ),所以首先通过tf.reshape方法将输入数据转换成(28, 28, 1)的矩阵(width=28，height=28,单通道) 0x0202 网络结构实验目的，网络结构比较简单，主要配置了两个卷基层，两个池化层，一个全连接层. - (3, 3, 1, 1) 卷积 - relu6激活函数 - max_pool 最大池化 - (3, 3, 1, 4) 卷积 - relu - max_pool 最大池化 - fc 全连接层 - dropout 抑制过拟合 - softmax 输出 卷积层conv2d_1，使用$3\\times3$大小的卷积核(vgg就是$3\\times3$)进行卷积操作，卷积输出通过relu6激活函数进行激活.这一层输出为(28, 28, 1)对卷积层conv2d_1 输出进行最大池化输出为(14,14,1)卷积层conv2d_2 使用$3\\times3$大小卷积核，feature map为4进行卷积操作， 并通过relu函数进行激活， 输出为(14,14, 4)对conv2d_2 进行最大池化，输出为（7， 7, 4)最后对conv2d_2输出进行全连接操作，全连接输出为1024维的特征向量，对该特征向量dropout正则化抑制过拟合. 最后通过softmax产生最终输出. 0x03 Tensorflow实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110#coding:utf-8import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataimport time\"\"\"权重初始化初始化为一个接近0的很小的正数\"\"\"def weight_variable(shape): initial = tf.truncated_normal(shape, stddev = 0.1) return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape = shape) return tf.Variable(initial)def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding = 'SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME') # tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC', name=None) # x(value) : [batch, height, width, channels] # ksize(pool大小) : A list of ints that has length &gt;= 4. The size of the window for each dimension of the input tensor. # strides(pool滑动大小) : A list of ints that has length &gt;= 4. The stride of the sliding window for each dimension of the input tensor.start = time.clock() #计算开始时间mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) #MNIST数据输入x = tf.placeholder(tf.float32,[None, 784])x_image = tf.reshape(x, [-1, 28, 28, 1]) #最后一维代表通道数目，如果是rgb则为3W_conv1 = weight_variable([3, 3, 1, 1])b_conv1 = bias_variable([1])h_conv1 = tf.nn.relu6(conv2d(x_image, W_conv1) + b_conv1)# x_image -&gt; [batch, in_height, in_width, in_channels]# [batch, 28, 28, 1]# W_conv1 -&gt; [filter_height, filter_width, in_channels, out_channels]# [5, 5, 1, 32]# output -&gt; [batch, out_height, out_width, out_channels]# [batch, 28, 28, 32]h_pool1 = max_pool_2x2(h_conv1)# h_conv1 -&gt; [batch, in_height, in_weight, in_channels]# [batch, 28, 28, 32]# output -&gt; [batch, out_height, out_weight, out_channels]# [batch, 14, 14, 32]\"\"\"第二层 卷积层h_pool1(batch, 14, 14, 32) -&gt; h_pool2(batch, 7, 7, 64)\"\"\"W_conv2 = weight_variable([3, 3, 1, 4])b_conv2 = bias_variable([1])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)# h_pool1 -&gt; [batch, 14, 14, 32]# W_conv2 -&gt; [5, 5, 32, 64]# output -&gt; [batch, 14, 14, 64]h_pool2 = max_pool_2x2(h_conv2)# h_conv2 -&gt; [batch, 14, 14, 64]# output -&gt; [batch, 7, 7, 64]\"\"\"第三层 全连接层h_pool2(batch, 7, 7, 64) -&gt; h_fc1(1, 1024)\"\"\"W_fc1 = weight_variable([7 * 7 * 4, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 4])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\"\"\"Dropouth_fc1 -&gt; h_fc1_drop, 训练中启用，测试中关闭\"\"\"keep_prob = tf.placeholder(\"float\")h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\"\"\"第四层 Softmax输出层\"\"\"W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\"\"\"训练和评估模型ADAM优化器来做梯度最速下降,feed_dict中加入参数keep_prob控制dropout比例\"\"\"y_ = tf.placeholder(\"float\", [None, 10])cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv)) #计算交叉熵with tf.name_scope(\"train\"): train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) #使用adam优化器来以0.0001的学习率来进行微调correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1)) #判断预测标签和实际标签是否匹配accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))sess = tf.Session() #启动创建的模型tf.summary.FileWriter(\"logs/\", sess.graph)sess.run(tf.initialize_all_variables()) #旧版本","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://blog.yindex.org/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"cnn","slug":"cnn","permalink":"https://blog.yindex.org/tags/cnn/"},{"name":"mnist","slug":"mnist","permalink":"https://blog.yindex.org/tags/mnist/"}]}]}